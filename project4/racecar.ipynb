{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ffeccb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|█████████████████████████████████| 1000/1000 [00:07<00:00, 126.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote summary to 29_QLrng_W-track.txt_STRT_metrics.txt\n",
      "Episodes logged: 1000\n",
      "Success rate: 1000/1000\n",
      "Steps per episode: mean = 302.26, min = 9, max = 6341\n",
      "Reward per episode: mean = -301.26, min = -6340.00, max = -8.00\n",
      "Crashes per episode: mean = 21.77, min = 0, max = 693\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import List,Tuple,TypeAlias,Set, Dict\n",
    "from enum import Enum, auto\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Parameters: This line is a must. The grader parser uses this line to locate the Parameters cell.\n",
    "GROUP_ID = 29\n",
    "ALGORITHM = 'QLrng'  # ValItr | QLrng | SARSA.\n",
    "TRACK_NAME = 'tracks/2-track.txt'\n",
    "CRASH_POS = 'STRT' # NRST | STRT\n",
    "\n",
    "FAIL_RATE = 0.2\n",
    "START_IDX = 0\n",
    "CRASH_SENSISTIVITY = 0.75\n",
    "\n",
    "# region Definitions and Setup\n",
    "Square: TypeAlias = Tuple[int, int]\n",
    "Vector: TypeAlias = Tuple[int, int]\n",
    "\n",
    "class SquareType(Enum):\n",
    "    START = auto()\n",
    "    FINISH = auto()\n",
    "    OPEN = auto()\n",
    "    WALL = auto()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "CHAR_TO_TOK = {\n",
    "    'S':SquareType.START,\n",
    "    'F':SquareType.FINISH,\n",
    "    '.':SquareType.OPEN,\n",
    "    '#':SquareType.WALL\n",
    "}\n",
    "\n",
    "TOK_TO_CHAR = {k:v for v,k in CHAR_TO_TOK.items()}\n",
    "\n",
    "SQUARE_COST = {\n",
    "    SquareType.START: 1,\n",
    "    SquareType.OPEN: 1,\n",
    "    SquareType.FINISH: 0,\n",
    "    SquareType.WALL: None\n",
    "}\n",
    "\n",
    "def bresenham_supercover(pos1: Square, pos2: Square) -> List[Square]:\n",
    "    x0, y0 = pos1\n",
    "    x1, y1 = pos2\n",
    "\n",
    "    dx = x1 - x0\n",
    "    dy = y1 - y0\n",
    "\n",
    "    sx = 1 if dx >= 0 else -1\n",
    "    sy = 1 if dy >= 0 else -1\n",
    "\n",
    "    dx = abs(dx)\n",
    "    dy = abs(dy)\n",
    "\n",
    "    x, y = x0, y0\n",
    "    points: List[Square] = [(x, y)]\n",
    "\n",
    "    if dx == 0 and dy == 0:\n",
    "        return points\n",
    "\n",
    "    if dx >= dy:\n",
    "        err = dx // 2\n",
    "        while x != x1:\n",
    "            err -= dy\n",
    "            if err < 0:\n",
    "                y += sy\n",
    "                points.append((x, y))\n",
    "                err += dx\n",
    "            x += sx\n",
    "            points.append((x, y))\n",
    "    else:\n",
    "        err = dy // 2\n",
    "        while y != y1:\n",
    "            err -= dx\n",
    "            if err < 0:\n",
    "                x += sx\n",
    "                points.append((x, y))\n",
    "                err += dy\n",
    "            y += sy\n",
    "            points.append((x, y))\n",
    "\n",
    "    return points\n",
    "\n",
    "def point_segment_distance(px, py, x1, y1, x2, y2):\n",
    "    APx = px - x1\n",
    "    APy = py - y1\n",
    "    ABx = x2 - x1\n",
    "    ABy = y2 - y1\n",
    "    mag2 = ABx * ABx + ABy * ABy\n",
    "    if mag2 == 0:\n",
    "        return math.hypot(APx, APy)\n",
    "    t = (APx * ABx + APy * ABy) / mag2\n",
    "    t = max(0.0, min(1.0, t))\n",
    "    cx = x1 + t * ABx\n",
    "    cy = y1 + t * ABy\n",
    "    return math.hypot(px - cx, py - cy)\n",
    "\n",
    "def bresenham_line(pos1: Square, pos2: Square, width: float = CRASH_SENSISTIVITY) -> List[Square]:\n",
    "    if width < 0:\n",
    "        raise ValueError(\"width must be >= 0\")\n",
    "\n",
    "    center_line = bresenham_supercover(pos1, pos2)\n",
    "\n",
    "    if width == 0.0:\n",
    "        return center_line\n",
    "\n",
    "    out: Set[Square] = set()\n",
    "\n",
    "    x1, y1 = pos1[0] + 0.5, pos1[1] + 0.5\n",
    "    x2, y2 = pos2[0] + 0.5, pos2[1] + 0.5\n",
    "\n",
    "    min_x = min(p[0] for p in center_line) - math.ceil(width)\n",
    "    max_x = max(p[0] for p in center_line) + math.ceil(width)\n",
    "    min_y = min(p[1] for p in center_line) - math.ceil(width)\n",
    "    max_y = max(p[1] for p in center_line) + math.ceil(width)\n",
    "\n",
    "    for x in range(min_x, max_x + 1):\n",
    "        for y in range(min_y, max_y + 1):\n",
    "            cx = x + 0.5\n",
    "            cy = y + 0.5\n",
    "            d = point_segment_distance(cx, cy, x1, y1, x2, y2)\n",
    "            if d <= width:\n",
    "                out.add((x, y))\n",
    "\n",
    "    return list(out)\n",
    "# endregion\n",
    "\n",
    "# region Track and Environment Classes\n",
    "class Track:\n",
    "    def __init__(self,filename=TRACK_NAME):\n",
    "        self.state: List[List[SquareType]] = []\n",
    "        self.start_squares: List[Square] = []\n",
    "        self.finish_squares: List[Square] = []\n",
    "        self.parse_track(filename)\n",
    "\n",
    "    def parse_track(self,track):\n",
    "        with open(track, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for row,line in enumerate(lines[1:]):\n",
    "                tok_line = []\n",
    "                for col,char in enumerate(line):\n",
    "                    if char=='\\n': continue\n",
    "                    tok = CHAR_TO_TOK[char]\n",
    "                    if tok == SquareType.START: self.start_squares.append((row,col))\n",
    "                    if tok == SquareType.FINISH: self.finish_squares.append((row,col))\n",
    "                    tok_line.append(tok)\n",
    "                self.state.append(tok_line)\n",
    "\n",
    "    def get_square(self,square: Square) -> SquareType:\n",
    "        return self.state[square[0]][square[1]]\n",
    "\n",
    "    def get_drivable_squares(self) -> List[Square]:\n",
    "        return [\n",
    "            (r, c)\n",
    "            for r, row in enumerate(self.state)\n",
    "            for c, col in enumerate(row)\n",
    "            if col != SquareType.WALL\n",
    "        ]\n",
    "\n",
    "    def get_start_squares(self) -> List[Square]:\n",
    "        return self.start_squares\n",
    "\n",
    "    def is_square_finish(self, square: Square) -> bool:\n",
    "        return self.get_square(square) == SquareType.FINISH\n",
    "\n",
    "    def is_square_drivable(self,square: Square) -> bool:\n",
    "        r, c = square\n",
    "        if r < 0 or r >= len(self.state): return False\n",
    "        if c < 0 or c >= len(self.state[0]): return False\n",
    "        return self.get_square(square) != SquareType.WALL\n",
    "\n",
    "\n",
    "class RaceTrackEnv:\n",
    "    def __init__(self, track: None|Track = None,starting_square: Square = None):\n",
    "        self.track: Track = track or Track()\n",
    "        self.position: Square = starting_square or self.track.start_squares[START_IDX]\n",
    "        self.velocity: Vector = (0,0)\n",
    "        self.acceleration: Vector = (0,0)\n",
    "\n",
    "    def stop(self):\n",
    "        self.acceleration = self.velocity = (0,0)\n",
    "\n",
    "    def reset(self, position: Square):\n",
    "        self.stop()\n",
    "        self.position = position\n",
    "\n",
    "    @staticmethod\n",
    "    def cap_velocity(velocity: Vector) -> Vector:\n",
    "        return tuple(min(5,max(-5,val)) for val in velocity)\n",
    "\n",
    "    def do_crash(self,position: Square,crash_position: str):\n",
    "        if crash_position == 'NRST':\n",
    "            nearest_start = min(\n",
    "                self.track.get_drivable_squares(),\n",
    "                key=lambda sq: (sq[0]-position[0])**2 + (sq[1]-position[1])**2\n",
    "            )\n",
    "            self.reset(nearest_start)\n",
    "        elif crash_position == 'STRT':\n",
    "            self.reset(self.track.start_squares[START_IDX])\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def check_crash(self,target_square: Square) -> Square|None:\n",
    "        path_points = bresenham_line(self.position, target_square)\n",
    "        for sq in path_points:\n",
    "            if not self.track.is_square_drivable(sq):\n",
    "                return sq\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def check_failure(fail_rate: float) -> bool:\n",
    "        return random.random() < fail_rate\n",
    "    \n",
    "    def check_finish(self) -> bool:\n",
    "        return self.track.is_square_finish(self.position)\n",
    "\n",
    "    def step(self,acceleration: Vector,fail_rate=FAIL_RATE,crash_position=CRASH_POS):\n",
    "        if not all(a in [-1,0,1] for a in acceleration):\n",
    "            raise ValueError\n",
    "        \n",
    "        do_accel = not self.check_failure(fail_rate)\n",
    "        if do_accel:\n",
    "            self.acceleration = acceleration\n",
    "        \n",
    "        self.velocity = self.cap_velocity(\n",
    "            (self.velocity[0]+self.acceleration[0],\n",
    "             self.velocity[1]+self.acceleration[1])\n",
    "        )\n",
    "        \n",
    "        target_position = (\n",
    "            self.position[0]+self.velocity[0],\n",
    "            self.position[1]+self.velocity[1]\n",
    "        )\n",
    "\n",
    "        crash = self.check_crash(target_position)\n",
    "        if not crash:\n",
    "            self.position = target_position\n",
    "        else:\n",
    "            self.do_crash(crash,crash_position)\n",
    "# endregion\n",
    "\n",
    "# region Model Based\n",
    "State: TypeAlias = Tuple[Square,Vector]\n",
    "\n",
    "class MDPModel:\n",
    "    def __init__(self, track: Track|None = None):\n",
    "        self.track = track or Track()\n",
    "        self.states: Set[State] = set([\n",
    "            (square, (vx, vy))\n",
    "            for square in self.track.get_drivable_squares()\n",
    "            for vx in range(-5, 6)\n",
    "            for vy in range(-5, 6)\n",
    "        ])\n",
    "        self.crash_cache: Dict[(Square,Square):Square] = {}\n",
    "        self.transitions: Dict[(State, Vector):List[(State, float)]] = {\n",
    "            (state,action):self.compute_transition_states_and_probs(state,action)\n",
    "            for state in tqdm(self.states,desc=\"Computing Transitions\")\n",
    "            for action in self.get_possible_actions()\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def cap_velocity(velocity: Vector) -> Vector:\n",
    "        return tuple(min(5,max(-5,val)) for val in velocity)\n",
    "\n",
    "    def check_crash(self, start_square: Square, target_square: Square) -> Square | None:\n",
    "        path = (start_square, target_square)\n",
    "        if path in self.crash_cache:\n",
    "            return self.crash_cache[path]\n",
    "\n",
    "        path_points = bresenham_line(start_square, target_square)\n",
    "        crash_square = None\n",
    "        for sq in path_points:\n",
    "            if crash_square is not None: break\n",
    "            crash_square = crash_square if self.track.is_square_drivable(sq) else sq\n",
    "\n",
    "        self.crash_cache[path] = crash_square\n",
    "        return crash_square\n",
    "\n",
    "    def do_crash(self, position: Square,crash_position: str):\n",
    "        if crash_position == 'NRST':\n",
    "            nearest_start = min(\n",
    "                self.track.get_drivable_squares(),\n",
    "                key=lambda sq: (sq[0]-position[0])**2 + (sq[1]-position[1])**2\n",
    "            )\n",
    "            return nearest_start\n",
    "        else:\n",
    "            return self.track.start_squares[START_IDX]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_possible_actions() -> List[Vector]:\n",
    "        return [(x,y) for x in [-1,0,1] for y in [-1,0,1]]\n",
    "\n",
    "    def compute_transition_states_and_probs(self, state: State, action,crash_position=CRASH_POS):\n",
    "        start_position,start_velocity = state\n",
    "\n",
    "        success_velocity = self.cap_velocity(\n",
    "            (start_velocity[0]+action[0], start_velocity[1]+action[1])\n",
    "        )\n",
    "        success_position = (\n",
    "            start_position[0]+success_velocity[0],\n",
    "            start_position[1]+success_velocity[1]\n",
    "        )\n",
    "        crash = self.check_crash(start_position, success_position)\n",
    "        if crash:\n",
    "            success_position = self.do_crash(crash,crash_position)\n",
    "            success_velocity = (0,0)\n",
    "\n",
    "        fail_velocity = self.cap_velocity(start_velocity)\n",
    "        fail_position = (\n",
    "            start_position[0] + fail_velocity[0],\n",
    "            start_position[1] + fail_velocity[1],\n",
    "        )\n",
    "        crash = self.check_crash(start_position, fail_position)\n",
    "        if crash:\n",
    "            fail_position = self.do_crash(crash, crash_position)\n",
    "            fail_velocity = (0, 0)\n",
    "\n",
    "        return [\n",
    "            ((fail_position,fail_velocity),FAIL_RATE),\n",
    "            ((success_position,success_velocity),1-FAIL_RATE)\n",
    "        ]\n",
    "\n",
    "    def get_transition_states_and_probs(self,state:State, action:Vector):\n",
    "        return self.transitions[(state,action)]\n",
    "\n",
    "    def get_cost(self, next_state):\n",
    "        return SQUARE_COST[self.track.get_square(next_state[0])]\n",
    "# endregion\n",
    "\n",
    "# region ValueIterationAgent\n",
    "class ValueIterationAgent:\n",
    "    def __init__(self, model: MDPModel | None = None,\n",
    "                 gamma: float = 0.9, theta: float = 1e-3):\n",
    "        self.value_table: dict = {}\n",
    "        self.policy: dict = {}\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "\n",
    "        if self.model is not None:\n",
    "            for s in self.model.states:\n",
    "                self.value_table[s] = 0.0\n",
    "            self.value_iteration()\n",
    "\n",
    "    def value_iteration(self, max_iterations: int = 1000):\n",
    "        assert self.model is not None\n",
    "        for s in self.model.states:\n",
    "            self.value_table.setdefault(s, 0.0)\n",
    "\n",
    "        delta0 = None\n",
    "\n",
    "        for _ in range(1, max_iterations + 1):\n",
    "            delta = 0.0\n",
    "\n",
    "            for s in self.model.states:\n",
    "                pos, vel = s\n",
    "\n",
    "                if self.model.track.is_square_finish(pos):\n",
    "                    new_v = 0.0\n",
    "                else:\n",
    "                    actions = self.model.get_possible_actions()\n",
    "                    best_q = float(\"-inf\")\n",
    "                    for a in actions:\n",
    "                        q = 0.0\n",
    "                        for next_state, prob in self.model.get_transition_states_and_probs(s, a):\n",
    "                            r = -self.model.get_cost(next_state)\n",
    "                            q += prob * (r + self.gamma * self.value_table.get(next_state, 0.0))\n",
    "                        if q > best_q:\n",
    "                            best_q = q\n",
    "                    new_v = best_q\n",
    "\n",
    "                delta = max(delta, abs(new_v - self.value_table.get(s, 0.0)))\n",
    "                self.value_table[s] = new_v\n",
    "\n",
    "            if delta0 is None:\n",
    "                delta0 = max(delta, 1e-12)\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "\n",
    "        self._extract_policy()\n",
    "\n",
    "    def _extract_policy(self):\n",
    "        assert self.model is not None\n",
    "        self.policy.clear()\n",
    "        for s in self.model.states:\n",
    "            pos, vel = s\n",
    "            if self.model.track.is_square_finish(pos):\n",
    "                continue\n",
    "            actions = self.model.get_possible_actions()\n",
    "\n",
    "            best_a = None\n",
    "            best_q = float(\"-inf\")\n",
    "            for a in actions:\n",
    "                q = 0.0\n",
    "                for next_state, prob in self.model.get_transition_states_and_probs(s, a):\n",
    "                    r = -self.model.get_cost(next_state)\n",
    "                    q += prob * (r + self.gamma * self.value_table.get(next_state, 0.0))\n",
    "                if q > best_q:\n",
    "                    best_q = q\n",
    "                    best_a = a\n",
    "            self.policy[s] = best_a\n",
    "\n",
    "    def get_action_for(self, state):\n",
    "        if state in self.policy:\n",
    "            return self.policy[state]\n",
    "        return None\n",
    "\n",
    "    def extract_greedy_path(self, max_steps: int = 1000) -> List[Square]:\n",
    "        start_square = self.model.track.start_squares[START_IDX]\n",
    "        state: State = (start_square, (0, 0))\n",
    "        path: List[Square] = [start_square]\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            pos, vel = state\n",
    "            if self.model.track.is_square_finish(pos):\n",
    "                break\n",
    "\n",
    "            action = self.get_action_for(state)\n",
    "            transitions = self.model.get_transition_states_and_probs(state, action)\n",
    "            next_state,_ = max(transitions, key=lambda item: item[1])\n",
    "\n",
    "            state = next_state\n",
    "            path.append(state[0])\n",
    "\n",
    "        return path\n",
    "# endregion\n",
    "\n",
    "# region SARSA + QLearning\n",
    "class SARSAAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=1.0,\n",
    "                 epsilon_min=0.05, epsilon_decay=0.995):\n",
    "        self.qtable = {}\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.actions = MDPModel.get_possible_actions()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def _q_value(self, state, action):\n",
    "        return self.qtable.get((state, action), 0.0)\n",
    "\n",
    "    def best_action(self, state):\n",
    "        best_q = float(\"-inf\")\n",
    "        best_actions = []\n",
    "        for a in self.actions:\n",
    "            q = self._q_value(state, a)\n",
    "            if q > best_q:\n",
    "                best_q = q\n",
    "                best_actions = [a]\n",
    "            elif q == best_q:\n",
    "                best_actions.append(a)\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(self.actions)\n",
    "        return self.best_action(state)\n",
    "\n",
    "    def update(self, state, action, reward, next_state, next_action, done=False):\n",
    "        old_q = self.qtable.get((state, action), 0.0)\n",
    "        if done or next_state is None or next_action is None:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * self._q_value(next_state, next_action)\n",
    "        self.qtable[(state, action)] = old_q + self.alpha * (target - old_q)\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.9,\n",
    "                 epsilon=1.0, epsilon_min=0.05, epsilon_decay=0.995):\n",
    "        self.qtable = {}\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.actions = MDPModel.get_possible_actions()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def _q_value(self, state, action):\n",
    "        return self.qtable.get((state, action), 0.0)\n",
    "\n",
    "    def best_action(self, state):\n",
    "        best_q = float(\"-inf\")\n",
    "        best_actions = []\n",
    "        for a in self.actions:\n",
    "            q = self._q_value(state, a)\n",
    "            if q > best_q:\n",
    "                best_q = q\n",
    "                best_actions = [a]\n",
    "            elif q == best_q:\n",
    "                best_actions.append(a)\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(self.actions)\n",
    "        return self.best_action(state)\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done=False):\n",
    "        old_q = self.qtable.get((state, action), 0.0)\n",
    "        if done or next_state is None:\n",
    "            target = reward\n",
    "        else:\n",
    "            max_next = max(self._q_value(next_state, a) for a in self.actions)\n",
    "            target = reward + self.gamma * max_next\n",
    "        self.qtable[(state, action)] = old_q + self.alpha * (target - old_q)\n",
    "# endregion\n",
    "\n",
    "# region Output and Metrics\n",
    "class EpisodeRunner:\n",
    "    def __init__(self, env: RaceTrackEnv, agent, max_steps: int = 1000):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "    def _current_state(self) -> State:\n",
    "        return (self.env.position, self.env.velocity)\n",
    "\n",
    "    def _reward(self) -> float:\n",
    "        square_type = self.env.track.get_square(self.env.position)\n",
    "        cost = SQUARE_COST[square_type]\n",
    "        return -float(cost)\n",
    "\n",
    "    def run_episode(self, start_square: Square | None = None):\n",
    "        if start_square is None:\n",
    "            start_square = self.env.track.start_squares[START_IDX]\n",
    "        self.env.reset(start_square)\n",
    "\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        crashes = 0\n",
    "\n",
    "        if isinstance(self.agent, SARSAAgent):\n",
    "            state = self._current_state()\n",
    "            action = self.agent.act(state)\n",
    "\n",
    "            for _ in range(self.max_steps):\n",
    "                prev_pos = self.env.position\n",
    "                self.env.step(action)\n",
    "                if self.env.position == prev_pos and not self.env.track.is_square_finish(self.env.position):\n",
    "                    crashes += 1\n",
    "\n",
    "                reward = self._reward()\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "                next_state = self._current_state()\n",
    "                done = self.env.track.is_square_finish(self.env.position)\n",
    "\n",
    "                if done:\n",
    "                    self.agent.update(state, action, reward, None, None, done=True)\n",
    "                    break\n",
    "\n",
    "                next_action = self.agent.act(next_state)\n",
    "                self.agent.update(state, action, reward, next_state, next_action, done=False)\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "        else:\n",
    "            for _ in range(self.max_steps):\n",
    "                state = self._current_state()\n",
    "                action = self.agent.act(state)\n",
    "\n",
    "                prev_pos = self.env.position\n",
    "                self.env.step(action)\n",
    "                if self.env.position == prev_pos and not self.env.track.is_square_finish(self.env.position):\n",
    "                    crashes += 1\n",
    "\n",
    "                reward = self._reward()\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "                next_state = self._current_state()\n",
    "                done = self.env.track.is_square_finish(self.env.position)\n",
    "\n",
    "                self.agent.update(state, action, reward, None if done else next_state, done=done)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "        return steps, total_reward, crashes\n",
    "\n",
    "\n",
    "class MetricsLogger:\n",
    "    def __init__(self):\n",
    "        self.episodes = []\n",
    "        self.steps = []\n",
    "        self.rewards = []\n",
    "        self.crashes = []\n",
    "        self.successes = 0\n",
    "\n",
    "    def log_episode(self, episode, steps, reward, crashes, finished):\n",
    "        self.episodes.append(episode)\n",
    "        self.steps.append(steps)\n",
    "        self.rewards.append(reward)\n",
    "        self.crashes.append(crashes)\n",
    "        if finished:\n",
    "            self.successes += 1\n",
    "\n",
    "    def print_metrics(self):\n",
    "        if not self.episodes:\n",
    "            print(\"No episodes logged.\")\n",
    "            return\n",
    "\n",
    "        n = len(self.episodes)\n",
    "        avg_steps = sum(self.steps) / n\n",
    "        avg_reward = sum(self.rewards) / n\n",
    "        avg_crashes = sum(self.crashes) / n\n",
    "\n",
    "        print(f\"Episodes logged: {n}\")\n",
    "        print(f\"Success rate: {self.successes}/{n}\")\n",
    "        print(f\"Steps per episode: mean = {avg_steps:.2f}, min = {min(self.steps)}, max = {max(self.steps)}\")\n",
    "        print(f\"Reward per episode: mean = {avg_reward:.2f}, min = {min(self.rewards):.2f}, max = {max(self.rewards):.2f}\")\n",
    "        print(f\"Crashes per episode: mean = {avg_crashes:.2f}, min = {min(self.crashes)}, max = {max(self.crashes)}\")\n",
    "\n",
    "    def write_summary(self, filename: str):\n",
    "        with open(filename, \"w\") as f:\n",
    "            n = len(self.episodes)\n",
    "            avg_steps = sum(self.steps) / n\n",
    "            avg_reward = sum(self.rewards) / n\n",
    "            avg_crashes = sum(self.crashes) / n\n",
    "\n",
    "            f.write(f\"Episodes logged: {n}\\n\")\n",
    "            f.write(f\"Success rate: {self.successes}/{n}\\n\")\n",
    "            f.write(f\"Avg steps: {avg_steps:.2f}\\n\")\n",
    "            f.write(f\"Min steps: {min(self.steps)}\\n\")\n",
    "            f.write(f\"Max steps: {max(self.steps)}\\n\")\n",
    "            f.write(f\"Avg reward: {avg_reward:.2f}\\n\")\n",
    "            f.write(f\"Min reward: {min(self.rewards):.2f}\\n\")\n",
    "            f.write(f\"Max reward: {max(self.rewards):.2f}\\n\")\n",
    "            f.write(f\"Avg crashes: {avg_crashes:.2f}\\n\")\n",
    "            f.write(f\"Min crashes: {min(self.crashes)}\\n\")\n",
    "            f.write(f\"Max crashes: {max(self.crashes)}\\n\")\n",
    "\n",
    "    def plot_path(self, track: Track, path: List[Square]):\n",
    "        n_rows = len(track.state)\n",
    "        n_cols = len(track.state[0])\n",
    "        grid = np.zeros((n_rows, n_cols))\n",
    "        for r, row in enumerate(track.state):\n",
    "            for c, cell in enumerate(row):\n",
    "                grid[r, c] = {SquareType.WALL: 0, SquareType.OPEN: 1,\n",
    "                              SquareType.START: 2, SquareType.FINISH: 3}[cell]\n",
    "\n",
    "        plt.figure(figsize=(7, 7))\n",
    "        plt.imshow(grid, origin=\"upper\")\n",
    "\n",
    "        for (r1, c1), (r2, c2) in zip(path[:-1], path[1:]):\n",
    "            plt.plot([c1, c2], [r1, r2], color=\"red\", linewidth=1)\n",
    "\n",
    "        (sr, sc) = path[0]\n",
    "        (er, ec) = path[-1]\n",
    "        for (r, c) in path:\n",
    "            plt.scatter([c], [r], s=20, color=\"yellow\")\n",
    "        plt.scatter([sc], [sr], s=80, color=\"green\")\n",
    "        plt.scatter([ec], [er], s=80, color=\"blue\")\n",
    "\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.title(\"Grid-Aligned Greedy Path\")\n",
    "        plt.savefig(f\"{GROUP_ID}_{ALGORITHM}_{TRACK_NAME.split('/')[-1]}_{CRASH_POS}.png\")\n",
    "        plt.show()\n",
    "# endregion\n",
    "\n",
    "def main():\n",
    "    track = Track(TRACK_NAME)\n",
    "\n",
    "    if ALGORITHM == 'ValItr':\n",
    "        model = MDPModel(track)\n",
    "        agent = ValueIterationAgent(model=model, gamma=0.9, theta=1e-3)\n",
    "\n",
    "        logger = MetricsLogger()\n",
    "        best_path = agent.extract_greedy_path()\n",
    "        print(\"Path: \", best_path)\n",
    "        logger.plot_path(track, best_path)\n",
    "\n",
    "    elif ALGORITHM == 'QLrng':\n",
    "        env = RaceTrackEnv(track)\n",
    "        agent = QLearningAgent(alpha=0.1, gamma=0.95, epsilon=0.1)\n",
    "        runner = EpisodeRunner(env, agent, max_steps=10_000)\n",
    "        logger = MetricsLogger()\n",
    "\n",
    "        n_episodes = 1_000\n",
    "        for ep in tqdm(range(1, n_episodes + 1), desc=\"Episodes\"):\n",
    "            steps, total_reward, crashes = runner.run_episode()\n",
    "            finished = env.track.is_square_finish(env.position)\n",
    "            logger.log_episode(ep, steps, total_reward, crashes, finished)\n",
    "            agent.decay_epsilon()\n",
    "\n",
    "        summary_name = f\"{GROUP_ID}_{ALGORITHM}_{TRACK_NAME.split('/')[-1]}_{CRASH_POS}_metrics.txt\"\n",
    "        logger.write_summary(summary_name)\n",
    "        print(f\"Wrote summary to {summary_name}\")\n",
    "        logger.print_metrics()\n",
    "\n",
    "        path = []\n",
    "        env.reset(track.start_squares[START_IDX])\n",
    "        for _ in range(1000):\n",
    "            pos = env.position\n",
    "            path.append(pos)\n",
    "            if track.is_square_finish(pos): break\n",
    "            state = (env.position, env.velocity)\n",
    "            action = agent.best_action(state)\n",
    "            env.step(action)\n",
    "\n",
    "        logger.plot_path(track, path)\n",
    "\n",
    "    elif ALGORITHM == 'SARSA':\n",
    "        env = RaceTrackEnv(track)\n",
    "        agent = SARSAAgent(alpha=0.1, gamma=0.975, epsilon=0.1)\n",
    "        runner = EpisodeRunner(env, agent, max_steps=10_000)\n",
    "        logger = MetricsLogger()\n",
    "\n",
    "        n_episodes = 1_000\n",
    "        for ep in tqdm(range(1, n_episodes + 1), desc=\"Episodes\"):\n",
    "            steps, total_reward, crashes = runner.run_episode()\n",
    "            finished = env.track.is_square_finish(env.position)\n",
    "            logger.log_episode(ep, steps, total_reward, crashes, finished)\n",
    "            agent.decay_epsilon()\n",
    "\n",
    "        summary_name = f\"{GROUP_ID}_{ALGORITHM}_{TRACK_NAME.split('/')[-1]}_{CRASH_POS}_metrics.txt\"\n",
    "        logger.write_summary(summary_name)\n",
    "        print(f\"Wrote summary to {summary_name}\")\n",
    "        logger.print_metrics()\n",
    "\n",
    "        path = []\n",
    "        env.reset(track.start_squares[START_IDX])\n",
    "        for _ in range(1000):\n",
    "            pos = env.position\n",
    "            path.append(pos)\n",
    "            if track.is_square_finish(pos): break\n",
    "            state = (env.position, env.velocity)\n",
    "            action = agent.best_action(state)\n",
    "            env.step(action)\n",
    "\n",
    "        logger.plot_path(track, path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown ALGORITHM: {ALGORITHM}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a70d63-3e9f-4117-bc3d-3c1403232c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
