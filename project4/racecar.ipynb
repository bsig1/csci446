{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffeccb6",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Transitions: 100%|██████████| 26136/26136 [00:22<00:00, 1180.99it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'initial_delta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 680\u001b[39m\n\u001b[32m    676\u001b[39m     logger.plot_path(track, best_path)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 666\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    664\u001b[39m track = Track(TRACK_NAME)\n\u001b[32m    665\u001b[39m model = MDPModel(track)\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m agent = \u001b[43mValueIterationAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    668\u001b[39m logger = MetricsLogger()\n\u001b[32m    670\u001b[39m \u001b[38;5;66;03m# (If you later run episodes with model-free methods, you'd call log_episode there.)\u001b[39;00m\n\u001b[32m    671\u001b[39m \u001b[38;5;66;03m# For VI, just visualize the greedy path once:\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 381\u001b[39m, in \u001b[36mValueIterationAgent.__init__\u001b[39m\u001b[34m(self, model, gamma, theta)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.states:\n\u001b[32m    380\u001b[39m     \u001b[38;5;28mself\u001b[39m.value_table[s] = \u001b[32m0.0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 416\u001b[39m, in \u001b[36mValueIterationAgent.value_iteration\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    413\u001b[39m     delta = \u001b[38;5;28mmax\u001b[39m(delta, \u001b[38;5;28mabs\u001b[39m(new_v - \u001b[38;5;28mself\u001b[39m.value_table.get(s, \u001b[32m0.0\u001b[39m)))\n\u001b[32m    414\u001b[39m     \u001b[38;5;28mself\u001b[39m.value_table[s] = new_v\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m progress = \u001b[32m1\u001b[39m - (delta / \u001b[43minitial_delta\u001b[49m)\n\u001b[32m    417\u001b[39m progress = \u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mmin\u001b[39m(progress, \u001b[32m1\u001b[39m))  \u001b[38;5;66;03m# clamp for safety\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Delta = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelta\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Progress: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogress*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'initial_delta' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import List,Tuple,TypeAlias,Set, Dict\n",
    "from enum import Enum, auto\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters: This line is a must. The grader parser uses this line to locate the Parameters cell.\n",
    "GROUP_ID = 29\n",
    "ALGORITHM = 'ValItr'  # ValItr | QLrng | SARSA. Note that “|” denotes a choice. Only one of the choices should be provided.\n",
    "TRACK_NAME = 'tracks/U-track.txt'\n",
    "CRASH_POS = 'NRST' # NRST | STRT\n",
    "\n",
    "\n",
    "FAIL_RATE = 0.2\n",
    "START_IDX = 0\n",
    "\n",
    "# region Definitions and Setup\n",
    "Square: TypeAlias = Tuple[int, int]\n",
    "Vector: TypeAlias = Tuple[int, int]\n",
    "\n",
    "class SquareType(Enum):\n",
    "    START = auto()       # starting square ('S')\n",
    "    FINISH = auto()      # finish square ('F')\n",
    "    OPEN = auto()        # open path ('.')\n",
    "    WALL = auto()        # wall ('#')\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "CHAR_TO_TOK = {\n",
    "    'S':SquareType.START,\n",
    "    'F':SquareType.FINISH,\n",
    "    '.':SquareType.OPEN,\n",
    "    '#':SquareType.WALL\n",
    "}\n",
    "\n",
    "TOK_TO_CHAR = {k:v for v,k in CHAR_TO_TOK.items()}\n",
    "\n",
    "SQUARE_COST = {\n",
    "    SquareType.START: 1,\n",
    "    SquareType.OPEN: 1,\n",
    "    SquareType.FINISH: 0,\n",
    "    SquareType.WALL: None\n",
    "}\n",
    "# endregion\n",
    "\n",
    "# region Track and Environment Classes\n",
    "class Track:\n",
    "    def __init__(self,filename=TRACK_NAME):\n",
    "        self.state: List[List[SquareType]] = []\n",
    "        self.start_squares: List[Square] = []\n",
    "        self.finish_squares: List[Square] = []\n",
    "\n",
    "        self.parse_track(filename)\n",
    "\n",
    "    def __str__(self):\n",
    "        out = \"\"\n",
    "        for row in self.state:\n",
    "            out += ''.join([TOK_TO_CHAR[tok] for tok in row])\n",
    "            out += '\\n'\n",
    "        return out[:-1]\n",
    "\n",
    "    def parse_track(self,track):\n",
    "        with open(track, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for row,line in enumerate(lines[1:]):\n",
    "                tok_line = []\n",
    "                for col,char in enumerate(line):\n",
    "                    if char=='\\n': continue\n",
    "                    \n",
    "                    tok = CHAR_TO_TOK[char]\n",
    "                    if tok == SquareType.START: self.start_squares.append((row,col))\n",
    "                    if tok == SquareType.FINISH: self.finish_squares.append((row,col))\n",
    "                    \n",
    "                    tok_line.append(tok)\n",
    "                \n",
    "                self.state.append(tok_line)\n",
    "\n",
    "    def get_square(self,square: Square) -> SquareType:\n",
    "        return self.state[square[0]][square[1]]\n",
    "\n",
    "    def get_drivable_squares(self) -> List[Square]:\n",
    "        \"\"\"\n",
    "        Returns all squares that are not walls\n",
    "        \"\"\"\n",
    "        return [\n",
    "            (r, c)\n",
    "            for r, row in enumerate(self.state)\n",
    "            for c, col in enumerate(row)\n",
    "            if col != SquareType.WALL\n",
    "        ]\n",
    "\n",
    "    def get_start_squares(self) -> List[Square]:\n",
    "        return self.start_squares\n",
    "\n",
    "    def is_square_finish(self, square: Square) -> bool:\n",
    "        return self.get_square(square) == SquareType.FINISH\n",
    "\n",
    "\n",
    "    def is_square_drivable(self,square: Square) -> bool:\n",
    "        r, c = square\n",
    "        if r < 0 or r >= len(self.state):\n",
    "            return False\n",
    "        if c < 0 or c >= len(self.state[0]):\n",
    "            return False\n",
    "        return self.get_square(square) != SquareType.WALL\n",
    "\n",
    "class RaceTrackEnv:\n",
    "    def __init__(self, track: None|Track = None,starting_square: Square = None):\n",
    "        self.track:        Track = track or Track()\n",
    "        self.position:     Square = starting_square or self.track.start_squares[START_IDX]\n",
    "        self.velocity:     Vector = (0,0)\n",
    "        self.acceleration: Vector = (0,0)\n",
    "\n",
    "    def stop(self):\n",
    "        self.acceleration = self.velocity = (0,0)\n",
    "\n",
    "    def reset(self, position: Square):\n",
    "        self.stop()\n",
    "        self.position = position\n",
    "\n",
    "    @staticmethod\n",
    "    def cap_velocity(velocity: Vector) -> Vector:\n",
    "        return tuple(min(5,max(-5,val)) for val in velocity)\n",
    "\n",
    "    @staticmethod\n",
    "    def bresenham_line(pos1: Square, pos2: Square) -> List[Square]:\n",
    "        \"\"\"Generate all points along a line using Bresenham's algorithm\"\"\"\n",
    "\n",
    "        points = []\n",
    "\n",
    "        x0, y0 = pos1\n",
    "        x1, y1 = pos2\n",
    "        dx, dy = abs(x1 - x0), abs(y1 - y0)\n",
    "        sx = 1 if x0 < x1 else -1\n",
    "        sy = 1 if y0 < y1 else -1\n",
    "        err = dx - dy\n",
    "\n",
    "        x, y = x0, y0\n",
    "        while True:\n",
    "            points.append((x, y))\n",
    "            if x == x1 and y == y1:\n",
    "                break\n",
    "            e2 = 2 * err\n",
    "            if e2 > -dy:\n",
    "                err -= dy\n",
    "                x += sx\n",
    "            if e2 < dx:\n",
    "                err += dx\n",
    "                y += sy\n",
    "\n",
    "        return points\n",
    "\n",
    "    def do_crash(self,position: Square,crash_position: str):\n",
    "        \"\"\"\n",
    "        Handles the crash based on the crash_position policy.\n",
    "        crash_position: 'NRST' | 'STRT'\n",
    "        1. 'NRST': Move to the nearest square.\n",
    "        2. 'STRT': Move to the starting square used at the beginning of the race\n",
    "        \"\"\"\n",
    "        if crash_position == 'NRST':\n",
    "            nearest_start = min(self.track.get_drivable_squares(), key=lambda sq: (sq[0]-position[0])**2 + (sq[1]-position[1])**2)\n",
    "            self.reset(nearest_start)\n",
    "        elif crash_position == 'STRT':\n",
    "            self.reset(self.track.start_squares[START_IDX])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid crash_position policy: {crash_position}\")\n",
    "\n",
    "    def check_crash(self,target_square: Square) -> Square|None:\n",
    "        \"\"\"\n",
    "        Check if moving along a line from current position to target crashes into an obstacle.\n",
    "        Uses Bresenham's line algorithm to trace the path.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get all points along the path\n",
    "        path_points = self.bresenham_line(\n",
    "            self.position, target_square\n",
    "        )\n",
    "        # Check each point for collision\n",
    "        for sq in path_points:\n",
    "            if not self.track.is_square_drivable(sq):\n",
    "                return sq  # Crash detected\n",
    "        return None  # No crash\n",
    "\n",
    "    @staticmethod\n",
    "    def check_failure(fail_rate: float) -> bool:\n",
    "        \"\"\"\n",
    "        Returns True if the action fails based on the fail_rate.\n",
    "        \"\"\"\n",
    "        if random.random() < fail_rate:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def check_finish(self) -> bool:\n",
    "        return self.track.is_square_finish(self.position)\n",
    "\n",
    "    def step(self,acceleration: Vector,fail_rate=FAIL_RATE,crash_position=CRASH_POS):\n",
    "        \"\"\"\n",
    "        Perform a step in the environment given an acceleration.\n",
    "        `Note velocity values are capped to [-5,5]`\n",
    "        acceleration: Tuple[int,int] where each value is in [-1,0,1]\n",
    "        fail_rate: Probability of action failure.\n",
    "        crash_position: 'NRST' | 'STRT' policy for handling crashes.\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        if not all(a in [-1,0,1] for a in acceleration):\n",
    "            raise ValueError(f\"Invalid acceleration: {acceleration}\")\n",
    "        \n",
    "        do_accel = True\n",
    "        if self.check_failure(fail_rate): do_accel = False\n",
    "        if do_accel:\n",
    "            self.acceleration = acceleration\n",
    "        \n",
    "        self.velocity = self.cap_velocity((self.velocity[0]+self.acceleration[0],self.velocity[1]+self.acceleration[1]))\n",
    "        target_position = (self.position[0]+self.velocity[0],self.position[1]+self.velocity[1])\n",
    "\n",
    "        crash = self.check_crash(target_position)\n",
    "        if not crash: self.position = target_position\n",
    "        else: self.do_crash(crash,crash_position)\n",
    "\n",
    "\n",
    "# endregion\n",
    "\n",
    "# region Model Based\n",
    "State: TypeAlias = Tuple[Square,Vector]\n",
    "\n",
    "class MDPModel:\n",
    "    \"\"\"\n",
    "    For use with ValueIterationAgent\n",
    "    \"\"\"\n",
    "    def __init__(self, track: Track|None = None):\n",
    "        # List / iterable of all states in the MDP\n",
    "        self.track = track or Track()\n",
    "        self.states: Set[State] = set([\n",
    "            (square, (vx, vy))\n",
    "            for square in self.track.get_drivable_squares()\n",
    "            for vx in range(-5, 6)\n",
    "            for vy in range(-5, 6)\n",
    "        ])\n",
    "        self.crash_cache: Dict[(Square,Square):Square] = {}\n",
    "        self.transitions: Dict[(State, Vector):List[(State, float)]] = {\n",
    "            (state,action):self.compute_transition_states_and_probs(state,action)\n",
    "            for state in tqdm(self.states,desc=\"Computing Transitions\")\n",
    "            for action in self.get_possible_actions()\n",
    "        }\n",
    "\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def cap_velocity(velocity: Vector) -> Vector:\n",
    "        return tuple(min(5,max(-5,val)) for val in velocity)\n",
    "\n",
    "    @staticmethod\n",
    "    def bresenham_line(pos1: Square, pos2: Square) -> List[Square]:\n",
    "        \"\"\"Generate all points along a line using Bresenham's algorithm\"\"\"\n",
    "\n",
    "        points = []\n",
    "\n",
    "        x0, y0 = pos1\n",
    "        x1, y1 = pos2\n",
    "        dx, dy = abs(x1 - x0), abs(y1 - y0)\n",
    "        sx = 1 if x0 < x1 else -1\n",
    "        sy = 1 if y0 < y1 else -1\n",
    "        err = dx - dy\n",
    "\n",
    "        x, y = x0, y0\n",
    "        while True:\n",
    "            points.append((x, y))\n",
    "            if x == x1 and y == y1:\n",
    "                break\n",
    "            e2 = 2 * err\n",
    "            if e2 > -dy:\n",
    "                err -= dy\n",
    "                x += sx\n",
    "            if e2 < dx:\n",
    "                err += dx\n",
    "                y += sy\n",
    "\n",
    "        return points\n",
    "\n",
    "    def check_crash(self, start_square: Square, target_square: Square) -> Square | None:\n",
    "        \"\"\"\n",
    "        Check if moving along a line from start_square to target_square crashes into an obstacle.\n",
    "        Uses Bresenham's line algorithm to trace the path.\n",
    "        Returns the first crash square, or None if no crash.\n",
    "        \"\"\"\n",
    "        path = (start_square, target_square)\n",
    "        if path in self.crash_cache:\n",
    "            return self.crash_cache[path]\n",
    "\n",
    "        path_points = self.bresenham_line(start_square, target_square)\n",
    "\n",
    "        crash_square = None\n",
    "        for sq in path_points:\n",
    "            if not self.track.is_square_drivable(sq):\n",
    "                crash_square = sq\n",
    "                break\n",
    "\n",
    "        self.crash_cache[path] = crash_square\n",
    "        return crash_square\n",
    "\n",
    "    def do_crash(self, position: Square,crash_position: str):\n",
    "        if crash_position == 'NRST':\n",
    "            nearest_start = min(self.track.get_drivable_squares(), key=lambda sq: (sq[0]-position[0])**2 + (sq[1]-position[1])**2)\n",
    "            return nearest_start\n",
    "        elif crash_position == 'STRT':\n",
    "            return self.track.start_squares[START_IDX]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid crash_position policy: {crash_position}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_possible_actions() -> List[Vector]:\n",
    "        \"\"\"\n",
    "        Actions are accelerations in this problem.\n",
    "        :return: a list of possible actions (acceleration values).\n",
    "        \"\"\"\n",
    "        return [(x,y) for x in [-1,0,1] for y in [-1,0,1]]\n",
    "\n",
    "    def compute_transition_states_and_probs(self, state: State, action,crash_position=CRASH_POS) -> List[Tuple[State, float]]:\n",
    "        \"\"\"\n",
    "        Return a list of (next_state, prob) pairs describing the transition\n",
    "        model P(s' | s, a).\n",
    "        \"\"\"\n",
    "        start_position,start_velocity = state\n",
    "\n",
    "        success_velocity = self.cap_velocity((start_velocity[0]+action[0],start_velocity[1]+action[1]))\n",
    "        success_position = (start_position[0]+success_velocity[0],start_position[1]+success_velocity[1])\n",
    "        crash = self.check_crash(start_position, success_position)\n",
    "        if crash:\n",
    "            success_position = self.do_crash(crash,crash_position)\n",
    "            success_velocity = (0,0)\n",
    "\n",
    "        fail_velocity = self.cap_velocity(start_velocity)\n",
    "        fail_position = (\n",
    "            start_position[0] + fail_velocity[0],\n",
    "            start_position[1] + fail_velocity[1],\n",
    "        )\n",
    "        crash = self.check_crash(start_position, fail_position)\n",
    "        if crash:\n",
    "            fail_position = self.do_crash(crash, crash_position)\n",
    "            fail_velocity = (0, 0)\n",
    "\n",
    "\n",
    "        return ([\n",
    "            ((fail_position,fail_velocity),FAIL_RATE),\n",
    "            ((success_position,success_velocity),1-FAIL_RATE)\n",
    "        ])\n",
    "\n",
    "    def get_transition_states_and_probs(self,state:State, action:Vector):\n",
    "        return self.transitions[(state,action)]\n",
    "\n",
    "    def get_cost(self, next_state):\n",
    "        \"\"\"\n",
    "        Return the immediate cost\n",
    "        \"\"\"\n",
    "        return SQUARE_COST[self.track.get_square(next_state[0])]\n",
    "\n",
    "\n",
    "class ValueIterationAgent:\n",
    "    \"\"\"\n",
    "    Classic value-iteration planner:\n",
    "    After convergence we extract a greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: MDPModel | None = None,\n",
    "                 gamma: float = 0.9,\n",
    "                 theta: float = 1e-3):\n",
    "        # Value function: dict[state] -> float\n",
    "        self.value_table: dict = {}\n",
    "        # Deterministic greedy policy: dict[state] -> action\n",
    "        self.policy: dict = {}\n",
    "\n",
    "        self.model: MDPModel | None = model\n",
    "        self.gamma: float = gamma\n",
    "        self.theta: float = theta  # convergence threshold\n",
    "\n",
    "        # If a model is already provided and has states, initialize and run VI\n",
    "        if self.model is not None and hasattr(self.model, \"states\"):\n",
    "            for s in self.model.states:\n",
    "                self.value_table[s] = 0.0\n",
    "            self.value_iteration()\n",
    "\n",
    "    def value_iteration(self):\n",
    "        assert self.model is not None, \"ValueIterationAgent: model is not set.\"\n",
    "\n",
    "        for s in getattr(self.model, \"states\", []):\n",
    "            self.value_table.setdefault(s, 0.0)\n",
    "\n",
    "        i = 0\n",
    "        initial_delta = None\n",
    "        while True:\n",
    "            delta = 0.0\n",
    "            i+=1\n",
    "            for s in self.model.states:\n",
    "                pos, vel = s\n",
    "                # Treat finish as terminal: value stays at 0\n",
    "                if self.model.track.is_square_finish(pos):\n",
    "                    new_v = 0.0\n",
    "                else:\n",
    "                    actions = self.model.get_possible_actions()\n",
    "                    if not actions:\n",
    "                        new_v = 0.0\n",
    "                    else:\n",
    "                        best_q = float(\"-inf\")\n",
    "                        for a in actions:\n",
    "                            q = 0.0\n",
    "                            for next_state, prob in self.model.get_transition_states_and_probs(s, a):\n",
    "                                r = -self.model.get_cost(next_state)\n",
    "                                q += prob * (r + self.gamma * self.value_table.get(next_state, 0.0))\n",
    "                            if q > best_q:\n",
    "                                best_q = q\n",
    "                        new_v = best_q\n",
    "\n",
    "                delta = max(delta, abs(new_v - self.value_table.get(s, 0.0)))\n",
    "                self.value_table[s] = new_v\n",
    "                if initial_delta is None:\n",
    "                    initial_delta = delta\n",
    "                \n",
    "            progress = 1 - (delta / initial_delta)\n",
    "            progress = max(0, min(progress, 1))  # clamp for safety\n",
    "\n",
    "            print(f\"Iter {i} | Delta = {delta:.6f} | Progress: {progress*100:.2f}%\")\n",
    "\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "        \n",
    "        self._extract_policy()\n",
    "\n",
    "\n",
    "    def _extract_policy(self):\n",
    "        assert self.model is not None, \"ValueIterationAgent: model is not set.\"\n",
    "\n",
    "        self.policy.clear()\n",
    "        for s in self.model.states:\n",
    "            pos, vel = s\n",
    "            if self.model.track.is_square_finish(pos):\n",
    "                continue  # terminal, no action\n",
    "\n",
    "            actions = self.model.get_possible_actions()\n",
    "            if not actions:\n",
    "                continue\n",
    "\n",
    "            best_a = None\n",
    "            best_q = float(\"-inf\")\n",
    "            for a in actions:\n",
    "                q = 0.0\n",
    "                for next_state, prob in self.model.get_transition_states_and_probs(s, a):\n",
    "                    r = self.model.get_cost(next_state)\n",
    "                    q += prob * (r + self.gamma * self.value_table.get(next_state, 0.0))\n",
    "                if q > best_q:\n",
    "                    best_q = q\n",
    "                    best_a = a\n",
    "\n",
    "            self.policy[s] = best_a\n",
    "\n",
    "\n",
    "    def get_action_for(self, state):\n",
    "        \"\"\"\n",
    "        Return the greedy action for `state` according to the current policy.\n",
    "        If the state is unknown, fall back to a simple default (None).\n",
    "        \"\"\"\n",
    "        # If we don't have an explicit policy entry, we can either:\n",
    "        #  - return None\n",
    "        #  - or compute a one-step greedy action on the fly\n",
    "        if state in self.policy:\n",
    "            return self.policy[state]\n",
    "\n",
    "        if self.model is None:\n",
    "            return None\n",
    "\n",
    "        actions = self.model.get_possible_actions()\n",
    "        if not actions:\n",
    "            return None\n",
    "\n",
    "        # One-step greedy backup for unseen states\n",
    "        best_a = None\n",
    "        best_q = float(\"-inf\")\n",
    "        for a in actions:\n",
    "            q = 0.0\n",
    "            for next_state, prob in self.model.get_transition_states_and_probs(state, a):\n",
    "                r = -self.model.get_cost(next_state)\n",
    "                q += prob * (r + self.gamma * self.value_table.get(next_state, 0.0))\n",
    "            if q > best_q:\n",
    "                best_q = q\n",
    "                best_a = a\n",
    "\n",
    "        return best_a\n",
    "\n",
    "    def extract_greedy_path(self, max_steps: int = 1000) -> List[Square]:\n",
    "        \"\"\"\n",
    "        Roll out the greedy policy from the first start square.\n",
    "        Returns a list of positions (Squares).\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "\n",
    "        start_square = self.model.track.start_squares[START_IDX]\n",
    "        state: State = (start_square, (0, 0))\n",
    "\n",
    "        path: List[Square] = [start_square]\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            pos, vel = state\n",
    "\n",
    "            # Stop if finish\n",
    "            if self.model.track.is_square_finish(pos):\n",
    "                break\n",
    "\n",
    "            action = self.get_action_for(state)\n",
    "\n",
    "            # Deterministic greedy next state\n",
    "            transitions = self.model.get_transition_states_and_probs(state, action)\n",
    "            next_state, _ = max(transitions, key=lambda item: item[1])\n",
    "\n",
    "            state = next_state\n",
    "            path.append(state[0])\n",
    "\n",
    "        return path\n",
    "\n",
    "\n",
    "    def stochastic_greedy_path(self, max_steps: int = 1000) -> List[Square]:\n",
    "        \"\"\"\n",
    "        Roll out the greedy policy from start_square\n",
    "        \"\"\"\n",
    "\n",
    "        state: State = (self.model.track.get_start_squares()[START_IDX], (0, 0))\n",
    "        path: List[Square] = [start_square]\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            pos, vel = state\n",
    "\n",
    "            # Stop if finish\n",
    "            if self.model.track.is_square_finish(pos):\n",
    "                break\n",
    "\n",
    "\n",
    "            action = self.get_action_for(state)\n",
    "            transitions = self.model.get_transition_states_and_probs(state, action)\n",
    "            next_state,_ = transitions[0] if random.random() < transitions[0][1] else transitions[1]\n",
    "\n",
    "            state = next_state\n",
    "            path.append(state[0]) \n",
    "        return path\n",
    "# endregion\n",
    "\n",
    "\n",
    "# region Model Free\n",
    "class SARSAAgent:\n",
    "    def __init__(self):\n",
    "        qtable = {}\n",
    "        alpha = 0\n",
    "        gamma = 0\n",
    "        epsilon = 0\n",
    "        last_state = None\n",
    "        last_action = None\n",
    "\n",
    "    def act(self,state):\n",
    "        pass\n",
    "\n",
    "    def update(self,state,action,reward,next_state,next_action):\n",
    "        pass\n",
    "\n",
    "    def best_action(self,state):\n",
    "        pass\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        qtable = {}\n",
    "        alpha = 0\n",
    "        gamma = 0\n",
    "        epsilon = 0\n",
    "\n",
    "    def act(self,state):\n",
    "        pass\n",
    "\n",
    "    def update(self,state,action,reward,next_state):\n",
    "        pass\n",
    "\n",
    "    def best_action(self,state):\n",
    "        pass\n",
    "# endregion\n",
    "\n",
    "# region Output and Metrics\n",
    "class EpisodeRunner:\n",
    "    def __init__(self):\n",
    "        env = None\n",
    "        agent = None\n",
    "        max_steps = 0\n",
    "\n",
    "    def run_episode(self,algorithm: SARSAAgent|QLearningAgent):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class MetricsLogger:\n",
    "    def __init__(self):\n",
    "        self.episodes: List[int] = []\n",
    "        self.steps: List[int] = []\n",
    "        self.rewards: List[float] = []\n",
    "\n",
    "    def log_episode(self, episode: int, steps: int, reward: float):\n",
    "        \"\"\"\n",
    "        Store metrics for a single episode.\n",
    "        \"\"\"\n",
    "        self.episodes.append(episode)\n",
    "        self.steps.append(steps)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def print_metrics(self):\n",
    "        \"\"\"\n",
    "        Print simple summary statistics over episodes.\n",
    "        \"\"\"\n",
    "        if not self.episodes:\n",
    "            print(\"No episodes logged.\")\n",
    "            return\n",
    "\n",
    "        n = len(self.episodes)\n",
    "        avg_steps = sum(self.steps) / n\n",
    "        avg_reward = sum(self.rewards) / n\n",
    "\n",
    "        print(f\"Episodes logged: {n}\")\n",
    "        print(f\"Steps per episode: mean = {avg_steps:.2f}, \"\n",
    "              f\"min = {min(self.steps)}, max = {max(self.steps)}\")\n",
    "        print(f\"Reward per episode: mean = {avg_reward:.2f}, \"\n",
    "              f\"min = {min(self.rewards):.2f}, max = {max(self.rewards):.2f}\")\n",
    "\n",
    "    def plot_path(self, track: Track, path: List[Square]):\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "\n",
    "        n_rows = len(track.state)\n",
    "        n_cols = len(track.state[0])\n",
    "\n",
    "        # Build grid image\n",
    "        grid = np.zeros((n_rows, n_cols))\n",
    "        for r, row in enumerate(track.state):\n",
    "            for c, cell in enumerate(row):\n",
    "                if cell == SquareType.WALL:\n",
    "                    grid[r, c] = 0\n",
    "                elif cell == SquareType.OPEN:\n",
    "                    grid[r, c] = 1\n",
    "                elif cell == SquareType.START:\n",
    "                    grid[r, c] = 2\n",
    "                elif cell == SquareType.FINISH:\n",
    "                    grid[r, c] = 3\n",
    "\n",
    "        plt.figure(figsize=(7, 7))\n",
    "        plt.imshow(grid, origin=\"upper\")\n",
    "\n",
    "        # Draw each step separately to avoid diagonal corner-cutting visuals\n",
    "        for (r1, c1), (r2, c2) in zip(path[:-1], path[1:]):\n",
    "            plt.plot([c1, c2], [r1, r2], color=\"red\", linewidth=2)\n",
    "\n",
    "        # Mark start and end\n",
    "        (sr, sc) = path[0]\n",
    "        (er, ec) = path[-1]\n",
    "        plt.scatter([sc], [sr], s=80, color=\"green\")   # start\n",
    "        plt.scatter([ec], [er], s=80, color=\"blue\")    # end\n",
    "\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.title(\"Grid-Aligned Greedy Path\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# endregion\n",
    "\n",
    "def main():\n",
    "    track = Track(TRACK_NAME)\n",
    "    model = MDPModel(track)\n",
    "    agent = ValueIterationAgent(model=model, gamma=0.9, theta=1e-3)\n",
    "\n",
    "    logger = MetricsLogger()\n",
    "\n",
    "    # (If you later run episodes with model-free methods, you'd call log_episode there.)\n",
    "    # For VI, just visualize the greedy path once:\n",
    "\n",
    "    start_square = track.start_squares[START_IDX]\n",
    "    best_path = agent.extract_greedy_path()\n",
    "\n",
    "    logger.plot_path(track, best_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
