{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T23:20:19.275667Z",
     "start_time": "2025-10-27T23:20:17.659250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disease:\n",
      "PFC TGA Fallot PAIVS TAPVD Lung\n",
      "0.03 0.30 0.29 0.27 0.08 0.04\n"
     ]
    }
   ],
   "source": [
    "from difflib import get_close_matches\n",
    "import csv\n",
    "from typing import Any, Optional, Set, Dict\n",
    "from dataclasses import dataclass, field\n",
    "from pgmpy.readwrite import BIFReader\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Parameters\n",
    "GROUP_ID = 29\n",
    "ALGORITHM = 'gibbs' # ’ve’ = Variable Elimination, ’gibbs’ = Gibbs Sampling\n",
    "NETWORK_NAME = './Networks/child.bif'\n",
    "REPORT = 'Disease' # e.g., Child: ’Disease’\n",
    "EVIDENCE_LEVEL = 'None' # {None | Little | Moderate} for Child/Insurance\n",
    "EVIDENCE = 'LowerBodyO2=<5; RUQO2=12+; CO2Report=>=7.5; XrayReport=Asy/Patchy'\n",
    "\n",
    "\n",
    "@dataclass (frozen=True)\n",
    "class Node:\n",
    "    # name is primary key\n",
    "    name: str\n",
    "    parents: tuple\n",
    "    values: tuple\n",
    "    probability_model: Optional[np.ndarray] = field(default=None, compare=False, repr=False)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.name)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Node):\n",
    "            return self.name == other.name\n",
    "        return False\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.name}: [{\", \".join(self.values)}]'\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, nodes: Set[Node] | None = None):\n",
    "        self.nodes: Set[Node] = nodes or set()\n",
    "        self.parents: Dict[Node, Set[Node]] = defaultdict(set)   # child -> {parents}\n",
    "        self.children: Dict[Node, Set[Node]] = defaultdict(set)  # parent -> {children}\n",
    "        self.by_name: Dict[str, Node] = {}\n",
    "\n",
    "    def add_node(self, node: Node):\n",
    "        if node.name not in self.by_name:\n",
    "            self.by_name[node.name] = node\n",
    "            self.nodes.add(node)\n",
    "\n",
    "    def add_edge(self, parent_name: str, child_name: str):\n",
    "        parent = self.by_name[parent_name]\n",
    "        child = self.by_name[child_name]\n",
    "        self.parents[child].add(parent)\n",
    "        self.children[parent].add(child)\n",
    "\n",
    "    def markov_blanket(self, node: Node) -> Set[Node]:\n",
    "        blanket: Set[Node] = self.parents.get(node, set())\n",
    "        children = self.children.get(node, set())\n",
    "        blanket.update(children)\n",
    "        # children's parents excluding self\n",
    "        for child in children:\n",
    "            blanket.update(parent for parent in self.parents.get(child, set()) if parent is not node)\n",
    "        blanket.discard(node)\n",
    "        return blanket\n",
    "\n",
    "    def degree(self, node: Node) -> int:\n",
    "        return len(self.parents[node])+len(self.children[node])\n",
    "\n",
    "    def topological_order(self):\n",
    "        degree_in_context = {node:len(self.parents[node]) for node in self.nodes}\n",
    "        node_queue = deque(sorted(self.nodes, key=lambda node: degree_in_context[node]))\n",
    "\n",
    "        output = []\n",
    "        while node_queue:\n",
    "            current_node = node_queue.pop()\n",
    "            if degree_in_context[current_node] == 0:\n",
    "                output.append(current_node)\n",
    "                for child in self.children[current_node]:\n",
    "                    degree_in_context[child] -= 1\n",
    "            else:\n",
    "                node_queue.appendleft(current_node)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join(str(n) for n in sorted(self.nodes, key=lambda n: n.name))\n",
    "\n",
    "\n",
    "class InputReader:\n",
    "    def __init__(self):\n",
    "        reader = BIFReader(NETWORK_NAME)\n",
    "        model = reader.get_model()\n",
    "        states = reader.get_states()\n",
    "        net = Network()\n",
    "\n",
    "        # 1) add nodes with CPDs reshaped to (child, *parents)\n",
    "        for variable in model.nodes():\n",
    "            cpd = model.get_cpds(variable)\n",
    "            parents = tuple(cpd.get_evidence() or tuple())\n",
    "            child_card = len(states[variable])\n",
    "            parent_cards = [len(states[p]) for p in parents]\n",
    "\n",
    "            pm_nd = np.array(cpd.values, dtype=float).reshape(\n",
    "                (child_card, *parent_cards),\n",
    "                order=\"F\"  # pgmpy: rightmost parent varies fastest\n",
    "            )\n",
    "\n",
    "            net.add_node(Node(\n",
    "                name=str(variable),\n",
    "                parents=parents,\n",
    "                values=tuple(states[variable]),\n",
    "                probability_model=pm_nd\n",
    "            ))\n",
    "\n",
    "        # 2) add edges\n",
    "        for child in model.nodes():\n",
    "            cpd = model.get_cpds(child)\n",
    "            for parent in (cpd.get_evidence() or []):\n",
    "                net.add_edge(str(parent), str(child))\n",
    "\n",
    "        self.network = net\n",
    "\n",
    "        # 3) parse EVIDENCE string\n",
    "        self.parsed_evidence = {}\n",
    "        for statement in EVIDENCE.split(\";\"):\n",
    "            i = statement.find('=')\n",
    "            key = statement[:i].strip()\n",
    "            value = statement[i+1:].strip()\n",
    "            if key not in self.network.by_name:\n",
    "                raise Exception(\"Invalid Evidence \" + key)\n",
    "            self.parsed_evidence[key] = value\n",
    "\n",
    "class Factor:\n",
    "    def __init__(self, variables, values):\n",
    "        \"\"\"\n",
    "        variables: list[str] in the order of axes of `values`\n",
    "        values: np.ndarray with one axis per variable (same order)\n",
    "        \"\"\"\n",
    "        self.variables = list(variables)\n",
    "        self.values = np.array(values, dtype=float)\n",
    "        assert self.values.ndim == len(self.variables), \\\n",
    "            \"values axes must match number of variables\"\n",
    "\n",
    "\n",
    "\n",
    "    def restrict(self, var, value_index):\n",
    "        if var not in self.variables:\n",
    "            return self\n",
    "        ax = self.variables.index(var)\n",
    "        # Take the slice and drop that axis\n",
    "        self.values = np.take(self.values, indices=value_index, axis=ax)\n",
    "        self.variables.pop(ax)\n",
    "        return self\n",
    "\n",
    "    def _align_to(self, all_vars):\n",
    "        \"\"\"\n",
    "        Return a view of self.values whose axes are placed at the positions\n",
    "        matching all_vars; missing variables are broadcast via singleton axes.\n",
    "        \"\"\"\n",
    "        # Where each of our variables should go in all_vars\n",
    "        dest_axes = [all_vars.index(v) for v in self.variables]\n",
    "\n",
    "        # Ensure we have enough axes to move: pad with singleton axes at the end\n",
    "        arr = self.values\n",
    "        need = len(all_vars) - arr.ndim\n",
    "        if need > 0:\n",
    "            arr = arr.reshape(arr.shape + (1,) * need)\n",
    "\n",
    "        # Move our current axes (0..k-1) to their destination positions\n",
    "        arr = np.moveaxis(arr, list(range(len(self.variables))), dest_axes)\n",
    "        return arr\n",
    "\n",
    "    def multiply(self, other):\n",
    "        \"\"\"\n",
    "        Pointwise multiply with broadcasting after aligning axes by variable name.\n",
    "        \"\"\"\n",
    "        # Stable union order: keep self order, then add other's new vars\n",
    "        all_vars = list(dict.fromkeys(self.variables + other.variables))\n",
    "\n",
    "        A = self._align_to(all_vars)\n",
    "        B = other._align_to(all_vars)\n",
    "\n",
    "        # Now A and B have the same ndim and compatible shapes\n",
    "        prod = A * B\n",
    "        return Factor(all_vars, prod)\n",
    "\n",
    "    def sum_out(self, var):\n",
    "        if var not in self.variables:\n",
    "            return self\n",
    "        ax = self.variables.index(var)\n",
    "        self.values = self.values.sum(axis=ax)\n",
    "        self.variables.pop(ax)\n",
    "        return self\n",
    "\n",
    "    def normalize(self):\n",
    "        Z = self.values.sum()\n",
    "        if Z != 0:\n",
    "            self.values = self.values / Z\n",
    "        return self\n",
    "\n",
    "\n",
    "class VESolver:\n",
    "\n",
    "    @staticmethod\n",
    "    def _value_index(node, label: str) -> int:\n",
    "        # exact or case-insensitive\n",
    "        try:\n",
    "            return node.values.index(label)\n",
    "        except ValueError:\n",
    "            lower_vals = [s.lower() for s in node.values]\n",
    "            try:\n",
    "                return lower_vals.index(label.lower())\n",
    "            except ValueError:\n",
    "                allowed = \", \".join(node.values)\n",
    "                raise ValueError(f\"Value {label!r} invalid for {node.name}. \"\n",
    "                                 f\"Allowed: [{allowed}]\")\n",
    "\n",
    "\n",
    "    # ---------- factor creation ----------\n",
    "    def _factor_from_node(self, network, node) -> \"Factor\":\n",
    "        \"\"\"\n",
    "        Reshape node.probability_model (pgmpy CPD values) from\n",
    "        (child_card, prod(parent_cards)) -> (child_card, *parent_cards)\n",
    "        following the exact parent order in node.parents.\n",
    "        \"\"\"\n",
    "        child = node.name\n",
    "        parents = list(node.parents or ())\n",
    "        variables = [child] + parents\n",
    "        child_card = len(network.by_name[child].values)\n",
    "        parent_cards = [len(network.by_name[p].values) for p in parents]\n",
    "\n",
    "        vals = np.array(node.probability_model, dtype=float).reshape(\n",
    "            (child_card, *parent_cards),\n",
    "            order=\"F\"  # pgmpy stores evidence columns in Fortran order\n",
    "        )\n",
    "        return Factor(variables, vals)\n",
    "\n",
    "    def _build_factors(self, network) -> list[\"Factor\"]:\n",
    "        return [self._factor_from_node(network, n) for n in network.nodes]\n",
    "\n",
    "    def solve(self, network, query: str, evidence: dict[str, str]):\n",
    "        # dict of labels: map to indices\n",
    "        evidence_node_states = {}\n",
    "\n",
    "        for variable in evidence:\n",
    "            evidence_node_states[variable] = network.by_name[variable].values.index(evidence[variable])\n",
    "\n",
    "        # factors\n",
    "        factors = self._build_factors(network)\n",
    "\n",
    "        # restrict evidence\n",
    "        for variable, state in evidence_node_states.items():\n",
    "            for factor in factors:\n",
    "                factor.restrict(variable, state)\n",
    "\n",
    "        elim_order = [node.name for node in network.topological_order() if node.name != query]\n",
    "\n",
    "        # eliminate\n",
    "        for eliminating_variable in elim_order:\n",
    "            bucket = [factor for factor in factors if eliminating_variable in factor.variables]\n",
    "            if not bucket:\n",
    "                continue\n",
    "            new_f = bucket[0]\n",
    "            for f in bucket[1:]:\n",
    "                new_f = new_f.multiply(f)\n",
    "            new_f.sum_out(eliminating_variable)\n",
    "            # replace bucket with new_f\n",
    "            factors = [f for f in factors if f not in bucket] + [new_f]\n",
    "\n",
    "        # multiply remaining & normalize\n",
    "        result = factors[0]\n",
    "        for f in factors[1:]:\n",
    "            result = result.multiply(f)\n",
    "        result.normalize()\n",
    "\n",
    "        # result should be a factor over [query] (possibly with size-1 evidence axes)\n",
    "        # If extra singleton axes remain, drop them until only query remains.\n",
    "        while result.variables != [query]:\n",
    "            # remove any size-1 axes by summing (no-op) or squeezing safely\n",
    "            for variable in list(result.variables):\n",
    "                if variable != query and result.values.shape[result.variables.index(variable)] == 1:\n",
    "                    result.sum_out(variable)\n",
    "                    break\n",
    "            else:\n",
    "                # ff we get here, there are still other vars present; multiply must have left something\n",
    "                break\n",
    "\n",
    "        print(REPORT+\":\")\n",
    "        print(\" \".join(network.by_name[REPORT].values))\n",
    "        print(\" \".join([f\"{val:.2f}\" for val in result.values]))\n",
    "        return result\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "class GibbsSolver:\n",
    "    \"\"\"\n",
    "    - network.by_name: dict[str, Node]\n",
    "    - network.parents: dict[Node, set[Node]]\n",
    "    - network.children: dict[Node, set[Node]]\n",
    "    - Node: name(str), parents(tuple[str]), values(tuple[str]), probability_model(np.ndarray)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, iterations=20000, burn_in=5000, thin=1, seed=None, verbose=False):\n",
    "        assert iterations > 0 and burn_in >= 0 and iterations > burn_in\n",
    "        assert thin >= 1\n",
    "        self.iterations = iterations\n",
    "        self.burn_in = burn_in\n",
    "        self.thin = thin\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def solve(self, network, report_var: str, evidence: dict[str, str]):\n",
    "        nodes_by_name = network.by_name                              # str -> Node\n",
    "        report_node = nodes_by_name[report_var]\n",
    "\n",
    "        state = {}\n",
    "        for name, node in nodes_by_name.items():\n",
    "            if name in evidence:\n",
    "                state[name] = evidence[name]\n",
    "            else:\n",
    "                state[name] = self.rng.choice(node.values)\n",
    "\n",
    "        non_evidence_nodes = [nodes_by_name[n] for n in nodes_by_name if n not in evidence]\n",
    "\n",
    "        tally = Counter()\n",
    "        kept = 0\n",
    "\n",
    "        for it in range(self.iterations):\n",
    "            for X_node in non_evidence_nodes:\n",
    "                X_name = X_node.name\n",
    "                x_vals = X_node.values\n",
    "                weights = np.empty(len(x_vals), dtype=float)\n",
    "\n",
    "                # Score each candidate value (Markov blanket go brrr)\n",
    "                for i, x in enumerate(x_vals):\n",
    "                    old_val = state[X_name]\n",
    "                    state[X_name] = x\n",
    "\n",
    "                    # p(X=x | Pa(X))\n",
    "                    w = self._cpd_prob(X_node, state, nodes_by_name)\n",
    "\n",
    "                    for Y_node in network.children.get(X_node, ()):\n",
    "                        w *= self._cpd_prob(Y_node, state, nodes_by_name)\n",
    "\n",
    "                    weights[i] = w if (w > 0 and np.isfinite(w)) else 0.0\n",
    "                    state[X_name] = old_val \n",
    "\n",
    "                s = weights.sum()\n",
    "                probs = (weights / s) if s > 0 else np.full(len(x_vals), 1.0 / len(x_vals))\n",
    "\n",
    "                # Samples new value for X\n",
    "                state[X_name] = self.rng.choice(x_vals, p=probs)\n",
    "\n",
    "            if it >= self.burn_in and ((it - self.burn_in) % self.thin == 0):\n",
    "                tally[state[report_var]] += 1\n",
    "                kept += 1\n",
    "\n",
    "        if kept == 0:\n",
    "            tally[state[report_var]] += 1\n",
    "            kept = 1\n",
    "\n",
    "        report_values = report_node.values\n",
    "        counts = np.array([tally[v] for v in report_values], dtype=float)\n",
    "        probs = counts / counts.sum()\n",
    "\n",
    "        print(report_var + \":\")\n",
    "        print(\" \".join(report_values))\n",
    "        print(\" \".join(f\"{p:.2f}\" for p in probs))\n",
    "\n",
    "        return probs.tolist()\n",
    "\n",
    "    def _cpd_prob(self, node, state, nodes_by_name):\n",
    "        \"\"\"\n",
    "        Return p(node = state[node.name] | parents(node) = state[...])\n",
    "        \"\"\"\n",
    "        pm = node.probability_model\n",
    "        par_names = node.parents or ()\n",
    "\n",
    "        # child outcome index\n",
    "        x_label = state[node.name]\n",
    "        try:\n",
    "            x_idx = node.values.index(x_label)\n",
    "        except ValueError:\n",
    "            return 0.0\n",
    "\n",
    "        # parent indices\n",
    "        par_sizes, par_indices = [], []\n",
    "        for p in par_names:\n",
    "            domain = nodes_by_name[p].values\n",
    "            pv = state[p]\n",
    "            try:\n",
    "                idx = domain.index(pv)\n",
    "            except ValueError:\n",
    "                return 0.0\n",
    "            par_sizes.append(len(domain))\n",
    "            par_indices.append(idx)\n",
    "\n",
    "        # Multi-dimensional case\n",
    "        if pm.ndim == 1 + len(par_names):\n",
    "            try:\n",
    "                return float(pm[(x_idx, *par_indices)])\n",
    "            except Exception:\n",
    "                return 0.0\n",
    "\n",
    "        # 2-D tabular case\n",
    "        if pm.ndim == 2:\n",
    "            if len(par_names) == 0:\n",
    "                if pm.shape[1] == 1:\n",
    "                    return float(pm[x_idx, 0])\n",
    "                if pm.shape[0] == len(node.values):\n",
    "                    return float(pm[x_idx])\n",
    "                return 0.0\n",
    "            try:\n",
    "                col = 0\n",
    "                for i, b in zip(reversed(par_indices), reversed(par_sizes)):\n",
    "                    col = col * b + i\n",
    "                return float(pm[x_idx, col])\n",
    "            except Exception:\n",
    "                return 0.0\n",
    "\n",
    "        # Unknown layout\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "\n",
    "class OutputWriter:\n",
    "    def __init__(self, output_text: str, network, algorithm: str):\n",
    "        os.makedirs(\"outputs\", exist_ok=True)\n",
    "        file_path = f\"outputs/{algorithm}_{REPORT}.txt\"\n",
    "\n",
    "        n_vars = len(network.nodes)\n",
    "        n_edges = sum(len(children) for children in network.children.values())\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(output_text)\n",
    "            f.write(f\"\\n{n_vars} variables\\n\")\n",
    "            f.write(f\"{n_edges} directed edges\\n\")\n",
    "\n",
    "class Driver:\n",
    "    def __init__(self):\n",
    "        self.reader = InputReader()\n",
    "        match(ALGORITHM.lower()):\n",
    "            case \"ve\":\n",
    "                self.solver = VESolver()\n",
    "            case \"gibbs\":\n",
    "                self.solver = GibbsSolver(iterations=50000, burn_in=10000, thin=5, seed=GROUP_ID)\n",
    "            case _:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        factor = self.solver.solve(self.reader.network,REPORT,self.reader.parsed_evidence)\n",
    "\n",
    "\n",
    "        result_text = f\"{REPORT}:\\n\" \\\n",
    "              f\"{' '.join(self.reader.network.by_name[REPORT].values)}\\n\" \\\n",
    "              f\"{' '.join(f'{val:.2f}' for val in factor.values if hasattr(factor, 'values')) if hasattr(factor, 'values') else ' '.join(f'{val:.2f}' for val in factor)}\\n\"\n",
    "\n",
    "        OutputWriter(result_text, self.reader.network, ALGORITHM)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Driver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aedd84-2bec-4424-b13f-ac5569d54a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI446 (Py3.11)",
   "language": "python",
   "name": "csci446-p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
