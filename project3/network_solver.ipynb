{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T04:37:19.336782Z",
     "start_time": "2025-11-03T04:36:50.770147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from difflib import get_close_matches\n",
    "import csv\n",
    "from typing import Any, Optional, Set, Dict, List, Union\n",
    "from dataclasses import dataclass, field\n",
    "from pgmpy.readwrite import BIFReader\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque, Counter\n",
    "import os\n",
    "\n",
    "# Parameters\n",
    "GROUP_ID = 29\n",
    "ALGORITHM = 'gibbs'\n",
    "NETWORK_NAME = './Networks/insurance.bif'\n",
    "REPORT = '[MedCost, ILiCost, PropCost]'\n",
    "EVIDENCE_LEVEL = 'None'\n",
    "EVIDENCE = \\\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "EVIDENCE = EVIDENCE.replace(\"\\n\",\"\")\n",
    "REPORT_DELIM = \",\"\n",
    "for char in \"“”\\\"\":\n",
    "    EVIDENCE = EVIDENCE.replace(char, \"\")\n",
    "\n",
    "@dataclass (frozen=True)\n",
    "class Node:\n",
    "    # name is primary key\n",
    "    name: str\n",
    "    parents: tuple\n",
    "    values: tuple\n",
    "    probability_model: Optional[np.ndarray] = field(default=None, compare=False, repr=False)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.name)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Node):\n",
    "            return self.name == other.name\n",
    "        return False\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.name}: [{\", \".join(self.values)}]'\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, nodes: Set[Node] | None = None):\n",
    "        self.nodes: Set[Node] = nodes or set()\n",
    "        self.parents: Dict[Node, Set[Node]] = defaultdict(set)   # child -> {parents}\n",
    "        self.children: Dict[Node, Set[Node]] = defaultdict(set)  # parent -> {children}\n",
    "        self.by_name: Dict[str, Node] = {}\n",
    "\n",
    "    def add_node(self, node: Node):\n",
    "        if node.name not in self.by_name:\n",
    "            self.by_name[node.name] = node\n",
    "            self.nodes.add(node)\n",
    "\n",
    "    def add_edge(self, parent_name: str, child_name: str):\n",
    "        parent = self.by_name[parent_name]\n",
    "        child = self.by_name[child_name]\n",
    "        self.parents[child].add(parent)\n",
    "        self.children[parent].add(child)\n",
    "\n",
    "    def markov_blanket(self, node: Node) -> Set[Node]:\n",
    "        blanket: Set[Node] = self.parents.get(node, set())\n",
    "        children = self.children.get(node, set())\n",
    "        blanket.update(children)\n",
    "        # children's parents excluding self\n",
    "        for child in children:\n",
    "            blanket.update(parent for parent in self.parents.get(child, set()) if parent is not node)\n",
    "        blanket.discard(node)\n",
    "        return blanket\n",
    "\n",
    "    def degree(self, node: Node) -> int:\n",
    "        return len(self.parents[node])+len(self.children[node])\n",
    "\n",
    "    def topological_order(self):\n",
    "        degree_in_context = {node:len(self.parents[node]) for node in self.nodes}\n",
    "        node_queue = deque(sorted(self.nodes, key=lambda node: degree_in_context[node]))\n",
    "\n",
    "        output = []\n",
    "        while node_queue:\n",
    "            current_node = node_queue.pop()\n",
    "            if degree_in_context[current_node] == 0:\n",
    "                output.append(current_node)\n",
    "                for child in self.children[current_node]:\n",
    "                    degree_in_context[child] -= 1\n",
    "            else:\n",
    "                node_queue.appendleft(current_node)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join(str(n) for n in sorted(self.nodes, key=lambda n: n.name))\n",
    "\n",
    "class InputReader:\n",
    "    def __init__(self):\n",
    "        reader = BIFReader(NETWORK_NAME)\n",
    "        model = reader.get_model()\n",
    "        states = reader.get_states()\n",
    "        net = Network()\n",
    "\n",
    "        # 1) add nodes with CPDs reshaped to (child, *parents)\n",
    "        for variable in model.nodes():\n",
    "            cpd = model.get_cpds(variable)\n",
    "\n",
    "            # Get parents in the SAME order as pgmpy stores them\n",
    "            parents_from_cpd = cpd.variables[1:]  # First is child, rest are parents\n",
    "            parents = tuple(str(p) for p in parents_from_cpd)  # Use pgmpy's order!\n",
    "\n",
    "            child_card = len(states[variable])\n",
    "            parent_cards = [len(states[p]) for p in parents]\n",
    "\n",
    "            pm_nd = np.array(cpd.values, dtype=float).reshape(\n",
    "                (child_card, *parent_cards),\n",
    "                order=\"F\"\n",
    "            )\n",
    "\n",
    "            net.add_node(Node(\n",
    "                name=str(variable),\n",
    "                parents=parents,\n",
    "                values=tuple(states[variable]),\n",
    "                probability_model=pm_nd\n",
    "            ))\n",
    "\n",
    "        # 2) add edges\n",
    "        for child in model.nodes():\n",
    "            cpd = model.get_cpds(child)\n",
    "            for parent in (cpd.get_evidence() or []):\n",
    "                net.add_edge(str(parent), str(child))\n",
    "\n",
    "        self.network = net\n",
    "\n",
    "        # 3) parse EVIDENCE string\n",
    "        self.parsed_evidence = {}\n",
    "        if EVIDENCE:\n",
    "            for statement in EVIDENCE.split(';'):\n",
    "                i = statement.find('=')\n",
    "                key = statement[:i].strip()\n",
    "                value = statement[i+1:].strip()\n",
    "                if key not in self.network.by_name:\n",
    "                    raise Exception(\"Invalid Evidence \" + key)\n",
    "                self.parsed_evidence[key] = value\n",
    "\n",
    "        # 4) parse report\n",
    "        self.parsed_report = []\n",
    "        self.parsed_report = REPORT\n",
    "        self.parsed_report = self.parsed_report[1:] if self.parsed_report.startswith(\"[\") else self.parsed_report\n",
    "        self.parsed_report = self.parsed_report[:-1] if self.parsed_report.endswith(\"]\") else self.parsed_report\n",
    "        self.parsed_report = [s.strip() for s in self.parsed_report.split(REPORT_DELIM)]\n",
    "\n",
    "class Factor:\n",
    "    def __init__(self, variables, values):\n",
    "\n",
    "        self.variables = list(variables)\n",
    "        self.values = np.array(values, dtype=np.float32)\n",
    "        assert self.values.ndim == len(self.variables), \\\n",
    "            f\"values.ndim ({self.values.ndim}) must equal len(variables) ({len(self.variables)}).\"\n",
    "\n",
    "    def reorder(self, vars_order):\n",
    "        \"\"\"Reorder axes to match vars_order.\"\"\"\n",
    "        if set(vars_order) != set(self.variables):\n",
    "            raise ValueError(\n",
    "                f\"reorder mismatch. have={self.variables}, want={vars_order}\"\n",
    "            )\n",
    "        source = list(range(len(self.variables)))\n",
    "        destination = [vars_order.index(v) for v in self.variables]\n",
    "        self.values = np.moveaxis(self.values, source, destination)\n",
    "        self.variables = list(vars_order)\n",
    "        return self\n",
    "\n",
    "    def restrict(self, var, value_index):\n",
    "        if var not in self.variables:\n",
    "            return self\n",
    "        ax = self.variables.index(var)\n",
    "        self.values = np.take(self.values, indices=value_index, axis=ax)\n",
    "        self.variables.pop(ax)\n",
    "        return self\n",
    "\n",
    "    def _align_to(self, all_vars):\n",
    "        \"\"\"Return a view to order all_vars.\"\"\"\n",
    "        dest_axes = [all_vars.index(v) for v in self.variables]\n",
    "\n",
    "        arr = self.values\n",
    "        need = len(all_vars) - arr.ndim\n",
    "        if need > 0:\n",
    "            arr = arr.reshape(arr.shape + (1,) * need)\n",
    "\n",
    "        arr = np.moveaxis(arr, list(range(len(self.variables))), dest_axes)\n",
    "        return arr\n",
    "\n",
    "    def multiply(self, other):\n",
    "        \"\"\"Pointwise multiply after aligning axes by variable name.\"\"\"\n",
    "        all_vars = list(dict.fromkeys(self.variables + other.variables))\n",
    "        A = self._align_to(all_vars)\n",
    "        B = other._align_to(all_vars)\n",
    "        prod = A * B  # numpy handles missing vars\n",
    "        return Factor(all_vars, prod)\n",
    "\n",
    "    def sum_out(self, var):\n",
    "        if var not in self.variables:\n",
    "            return self\n",
    "        ax = self.variables.index(var)\n",
    "        self.values = self.values.sum(axis=ax)\n",
    "        self.variables.pop(ax)\n",
    "        return self\n",
    "\n",
    "    def normalize(self):\n",
    "        Z = self.values.sum()\n",
    "        if Z != 0:\n",
    "            self.values = self.values / Z\n",
    "        # else: leave as-is\n",
    "        return self\n",
    "\n",
    "class VESolver:\n",
    "    @staticmethod\n",
    "    def _value_index(node, label: str) -> int:\n",
    "        try:\n",
    "            return node.values.index(label)\n",
    "        except ValueError:\n",
    "            lower_vals = [s.lower() for s in node.values]\n",
    "            try:\n",
    "                return lower_vals.index(label.lower())\n",
    "            except ValueError:\n",
    "                allowed = \", \".join(node.values)\n",
    "                raise ValueError(\n",
    "                    f\"Value {label!r} invalid for {node.name}. Allowed: [{allowed}]\"\n",
    "                )\n",
    "\n",
    "    def _factor_from_node(self, network, node) -> \"Factor\":\n",
    "        \"\"\"\n",
    "        Assumes CPD stored as shape (child, *parents)\n",
    "        \"\"\"\n",
    "        child = node.name\n",
    "        parents = list(node.parents or ())\n",
    "        variables = [child] + parents\n",
    "        vals = np.array(node.probability_model, dtype=float, copy=True)\n",
    "        return Factor(variables, vals)\n",
    "\n",
    "    def _build_factors(self, network) -> list[\"Factor\"]:\n",
    "        return [self._factor_from_node(network, n) for n in network.nodes]\n",
    "\n",
    "    def _ancestors_of(self, network, vars_set: set[str]) -> set[str]:\n",
    "        \"\"\"\n",
    "        Return vars_set ∪ all their (recursive) parents\n",
    "        \"\"\"\n",
    "        keep = set(vars_set)\n",
    "        stack = list(vars_set)\n",
    "        while stack:\n",
    "            v = stack.pop()\n",
    "            node = network.by_name[v]\n",
    "            for p in (node.parents or ()):\n",
    "                if p not in keep:\n",
    "                    keep.add(p)\n",
    "                    stack.append(p)\n",
    "        return keep\n",
    "\n",
    "    def _build_factors_for(self, network, keep_vars: set[str]) -> list[\"Factor\"]:\n",
    "        \"\"\"Only build factors for nodes whose variable is in keep_vars.\"\"\"\n",
    "\n",
    "        return [\n",
    "            self._factor_from_node(network, n)\n",
    "            for n in network.nodes\n",
    "            if n.name in keep_vars\n",
    "        ]\n",
    "\n",
    "    def solve(self, network, query: List[str] | str, evidence: dict[str, str]):\n",
    "        if isinstance(query, str):\n",
    "            query = [query]\n",
    "\n",
    "        if query[0] in evidence:\n",
    "            node = network.by_name[query[0]]\n",
    "            num_values = len(node.values)\n",
    "            return Factor([query[0]], [-1]*num_values)\n",
    "\n",
    "        # Map evidence labels, indices (case-insensitive, validated)\n",
    "        evidence_node_states = {}\n",
    "        for variable, label in evidence.items():\n",
    "            node = network.by_name[variable]\n",
    "            evidence_node_states[variable] = self._value_index(node, label)\n",
    "\n",
    "\n",
    "        frontier = set(query) | set(evidence.keys())\n",
    "        keep_vars = self._ancestors_of(network, frontier) | frontier\n",
    "\n",
    "\n",
    "        # Build and restrict factors (only ancestors kept)\n",
    "        factors = self._build_factors_for(network, keep_vars)\n",
    "        for variable, idx in evidence_node_states.items():\n",
    "            for factor in factors:\n",
    "                factor.restrict(variable, idx)\n",
    "\n",
    "        # Elimination order: topological over kept vars, drop evidence & query\n",
    "        elim_order = [\n",
    "            node.name for node in network.topological_order()\n",
    "            if node.name in keep_vars\n",
    "            and node.name not in evidence\n",
    "            and node.name not in query\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Variable elimination\n",
    "        for elim in elim_order:\n",
    "            bucket = [f for f in factors if elim in f.variables]\n",
    "            if not bucket:\n",
    "                continue\n",
    "            new_factor = Factor(bucket[0].variables[:],\n",
    "                                np.array(bucket[0].values, dtype=np.float32, copy=True))\n",
    "            for f in bucket[1:]:\n",
    "                new_factor = new_factor.multiply(f)\n",
    "            new_factor.sum_out(elim)\n",
    "            factors = [f for f in factors if f not in bucket] + [new_factor]\n",
    "\n",
    "\n",
    "        # Multiply remaining factors\n",
    "        if not factors:\n",
    "            raise RuntimeError(\"No factors remain after elimination.\")\n",
    "        result = factors[0]\n",
    "        for f in factors[1:]:\n",
    "            result = result.multiply(f)\n",
    "\n",
    "        # Sum out everything not in query\n",
    "        for v in list(result.variables):\n",
    "            if v not in query:\n",
    "                result.sum_out(v)\n",
    "\n",
    "        # Reorder + normalize\n",
    "        result.reorder(query).normalize()\n",
    "        return result\n",
    "\n",
    "class GibbsSolver:\n",
    "    \"\"\"\n",
    "    - network.by_name: dict[str, Node]\n",
    "    - network.parents: dict[Node, set[Node]]\n",
    "    - network.children: dict[Node, set[Node]]\n",
    "    - Node: name(str), parents(tuple[str]), values(tuple[str]), probability_model(np.ndarray)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, iterations=2000, burn_in=500, thin=1, seed=None, verbose=False):\n",
    "        assert iterations > 0 and 0 <= burn_in < iterations\n",
    "        assert thin >= 1\n",
    "        self.iterations = iterations\n",
    "        self.burn_in = burn_in\n",
    "        self.thin = thin\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def solve(self, network, report_var: str, evidence: dict[str, str]):\n",
    "\n",
    "        if report_var in evidence:\n",
    "            node = network.by_name[report_var]\n",
    "            num_values = len(node.values)\n",
    "            return Factor([report_var], [-1]*num_values)\n",
    "\n",
    "        nodes_by_name = network.by_name                              # str -> Node\n",
    "        report_node = nodes_by_name[report_var]\n",
    "\n",
    "        state = {}\n",
    "        for name, node in nodes_by_name.items():\n",
    "            if name in evidence:\n",
    "                state[name] = evidence[name]\n",
    "            else:\n",
    "                state[name] = self.rng.choice(node.values)\n",
    "\n",
    "        non_evidence_nodes = [nodes_by_name[n] for n in nodes_by_name if n not in evidence]\n",
    "\n",
    "        tally = Counter()\n",
    "        kept = 0\n",
    "        try:\n",
    "            from tqdm import tqdm\n",
    "            for it in tqdm(range(self.iterations), desc=f\"Gibbs sampling: {report_var}\"):\n",
    "                for X_node in non_evidence_nodes:\n",
    "                    X_name = X_node.name\n",
    "                    x_vals = X_node.values\n",
    "                    weights = np.empty(len(x_vals), dtype=float)\n",
    "\n",
    "                    # Score each candidate value (Markov blanket go brrr)\n",
    "                    for i, x in enumerate(x_vals):\n",
    "                        old_val = state[X_name]\n",
    "                        state[X_name] = x\n",
    "\n",
    "                        # p(X=x | Pa(X))\n",
    "                        w = self._cpd_prob(X_node, state, nodes_by_name)\n",
    "\n",
    "                        for Y_node in network.children.get(X_node, ()):\n",
    "                            w *= self._cpd_prob(Y_node, state, nodes_by_name)\n",
    "\n",
    "                        weights[i] = w if (w > 0 and np.isfinite(w)) else 0.0\n",
    "                        state[X_name] = old_val\n",
    "\n",
    "                    s = weights.sum()\n",
    "                    probs = (weights / s) if s > 0 else np.full(len(x_vals), 1.0 / len(x_vals))\n",
    "\n",
    "                    # Samples new value for X\n",
    "                    state[X_name] = self.rng.choice(x_vals, p=probs)\n",
    "\n",
    "                if it >= self.burn_in and ((it - self.burn_in) % self.thin == 0):\n",
    "                    tally[state[report_var]] += 1\n",
    "                    kept += 1\n",
    "        except Exception:\n",
    "            for it in range(self.iterations):\n",
    "                for X_node in non_evidence_nodes:\n",
    "                    X_name = X_node.name\n",
    "                    x_vals = X_node.values\n",
    "                    weights = np.empty(len(x_vals), dtype=float)\n",
    "\n",
    "                    # Score each candidate value (Markov blanket go brrr)\n",
    "                    for i, x in enumerate(x_vals):\n",
    "                        old_val = state[X_name]\n",
    "                        state[X_name] = x\n",
    "\n",
    "                        # p(X=x | Pa(X))\n",
    "                        w = self._cpd_prob(X_node, state, nodes_by_name)\n",
    "\n",
    "                        for Y_node in network.children.get(X_node, ()):\n",
    "                            w *= self._cpd_prob(Y_node, state, nodes_by_name)\n",
    "\n",
    "                        weights[i] = w if (w > 0 and np.isfinite(w)) else 0.0\n",
    "                        state[X_name] = old_val\n",
    "\n",
    "                    s = weights.sum()\n",
    "                    probs = (weights / s) if s > 0 else np.full(len(x_vals), 1.0 / len(x_vals))\n",
    "\n",
    "                    # Samples new value for X\n",
    "                    state[X_name] = self.rng.choice(x_vals, p=probs)\n",
    "\n",
    "                if it >= self.burn_in and ((it - self.burn_in) % self.thin == 0):\n",
    "                    tally[state[report_var]] += 1\n",
    "                    kept += 1\n",
    "\n",
    "        if kept == 0:\n",
    "            tally[state[report_var]] += 1\n",
    "            kept = 1\n",
    "\n",
    "        report_values = report_node.values\n",
    "        counts = np.array([tally[v] for v in report_values], dtype=float)\n",
    "        probs = counts / counts.sum()\n",
    "\n",
    "        return Factor([report_var], np.array(probs, dtype=np.float32))\n",
    "\n",
    "    def _cpd_prob(self, node, state, nodes_by_name):\n",
    "        \"\"\"\n",
    "        Return p(node = state[node.name] | parents(node) = state[...])\n",
    "        \"\"\"\n",
    "        pm = node.probability_model\n",
    "        par_names = node.parents or ()\n",
    "\n",
    "        # child outcome index\n",
    "        x_label = state[node.name]\n",
    "        try:\n",
    "            x_idx = node.values.index(x_label)\n",
    "        except ValueError:\n",
    "            return 0.0\n",
    "\n",
    "        # parent indices\n",
    "        par_sizes, par_indices = [], []\n",
    "        for p in par_names:\n",
    "            domain = nodes_by_name[p].values\n",
    "            pv = state[p]\n",
    "            try:\n",
    "                idx = domain.index(pv)\n",
    "            except ValueError:\n",
    "                return 0.0\n",
    "            par_sizes.append(len(domain))\n",
    "            par_indices.append(idx)\n",
    "\n",
    "        # Multi-dimensional case\n",
    "        if pm.ndim == 1 + len(par_names):\n",
    "            try:\n",
    "                return float(pm[(x_idx, *par_indices)])\n",
    "            except Exception:\n",
    "                return 0.0\n",
    "\n",
    "        # 2-D tabular case\n",
    "        if pm.ndim == 2:\n",
    "            if len(par_names) == 0:\n",
    "                if pm.shape[1] == 1:\n",
    "                    return float(pm[x_idx, 0])\n",
    "                if pm.shape[0] == len(node.values):\n",
    "                    return float(pm[x_idx])\n",
    "                return 0.0\n",
    "            try:\n",
    "                col = 0\n",
    "                for i, b in zip(reversed(par_indices), reversed(par_sizes)):\n",
    "                    col = col * b + i\n",
    "                return float(pm[x_idx, col])\n",
    "            except Exception:\n",
    "                return 0.0\n",
    "\n",
    "        # Unknown layout\n",
    "        return 0.0\n",
    "\n",
    "class OutputWriter:\n",
    "    def __init__(self, output: List[List[Union[str, int]]]):\n",
    "        os.makedirs(\"outputs\", exist_ok=True)\n",
    "        name = os.path.splitext(os.path.basename(NETWORK_NAME))[0]\n",
    "        file_path = f\"outputs/group{GROUP_ID}_{ALGORITHM}_{name}_{EVIDENCE_LEVEL}.csv\"\n",
    "        print(f\"Writing to {file_path}\")\n",
    "\n",
    "        with open(file_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(output)\n",
    "\n",
    "class Driver:\n",
    "    def __init__(self):\n",
    "        self.reader = InputReader()\n",
    "        match(ALGORITHM.lower()):\n",
    "            case \"ve\":\n",
    "                self.solver = VESolver()\n",
    "            case \"gibbs\":\n",
    "                self.solver = GibbsSolver(iterations=10_000, burn_in=1000, thin=5, seed=GROUP_ID)\n",
    "            case _:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        marginals = [self.solver.solve(self.reader.network,report,self.reader.parsed_evidence) for report in self.reader.parsed_report]\n",
    "        output = []\n",
    "        for f in marginals:\n",
    "            var = f.variables[0]\n",
    "            output.append([var] + list(self.reader.network.by_name[var].values))\n",
    "            if all([i == -1 for i in f.values]):\n",
    "                output.append(['x']*len(f.values))\n",
    "            else:\n",
    "                output.append([f\"{n:.2f}\" for n in f.values])\n",
    "\n",
    "\n",
    "        for line in output:\n",
    "            print(\", \".join(line))\n",
    "\n",
    "        OutputWriter(output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    driver = Driver()"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gibbs sampling: MedCost: 100%|██████████| 10000/10000 [00:08<00:00, 1207.43it/s]\n",
      "Gibbs sampling: ILiCost: 100%|██████████| 10000/10000 [00:08<00:00, 1242.54it/s]\n",
      "Gibbs sampling: PropCost: 100%|██████████| 10000/10000 [00:08<00:00, 1234.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9216667  0.03222222 0.02611111 0.02      ]\n",
      "[0.9811111  0.01       0.00555556 0.00333333]\n",
      "[0.61777776 0.3061111  0.06944445 0.00666667]\n",
      "MedCost, Thousand, TenThou, HundredThou, Million\n",
      "0.92, 0.03, 0.03, 0.02\n",
      "ILiCost, Thousand, TenThou, HundredThou, Million\n",
      "0.98, 0.01, 0.01, 0.00\n",
      "PropCost, Thousand, TenThou, HundredThou, Million\n",
      "0.62, 0.31, 0.07, 0.01\n",
      "Writing to outputs/group29_gibbs_insurance_None.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
