{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T04:37:19.336782Z",
     "start_time": "2025-11-03T04:36:50.770147Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gibbs sampling: Problem2: 100%|██████████| 20000/20000 [00:48<00:00, 416.38it/s]\n",
      "Gibbs sampling: Problem3: 100%|██████████| 20000/20000 [00:47<00:00, 416.92it/s]\n",
      "Gibbs sampling: Problem4: 100%|██████████| 20000/20000 [00:47<00:00, 417.48it/s]\n",
      "Gibbs sampling: Problem5: 100%|██████████| 20000/20000 [00:48<00:00, 415.68it/s]\n",
      "Gibbs sampling: Problem6: 100%|██████████| 20000/20000 [00:48<00:00, 414.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem1, Normal_Output, No_Output\n",
      "x, x\n",
      "Problem2, OK, Too_Long\n",
      "1.00, 0.00\n",
      "Problem3, No, Yes\n",
      "0.03, 0.97\n",
      "Problem4, No, Yes\n",
      "0.06, 0.94\n",
      "Problem5, No, Yes\n",
      "0.05, 0.95\n",
      "Problem6, No, Yes\n",
      "1.00, 0.00\n",
      "Writing to outputs/group29_gibbs_win95pts_None.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from difflib import get_close_matches\n",
    "import csv\n",
    "from typing import Any, Optional, Set, Dict, List, Union\n",
    "from dataclasses import dataclass, field\n",
    "from pgmpy.readwrite import BIFReader\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque, Counter\n",
    "import os\n",
    "\n",
    "# Parameters\n",
    "GROUP_ID = 29\n",
    "ALGORITHM = 'gibbs'\n",
    "NETWORK_NAME = './Networks/win95pts.bif'\n",
    "REPORT = '[Problem1, Problem2, Problem3, Problem4, Problem5, Problem6]'\n",
    "EVIDENCE_LEVEL = 'None'\n",
    "EVIDENCE = 'Problem1=No_Output'\n",
    "\n",
    "\n",
    "EVIDENCE = EVIDENCE.replace(\"\\n\",\"\")\n",
    "REPORT_DELIM = \",\"\n",
    "for char in \"“”\\\"\":\n",
    "    EVIDENCE = EVIDENCE.replace(char, \"\")\n",
    "\n",
    "@dataclass (frozen=True)\n",
    "class Node:\n",
    "    # name is primary key\n",
    "    name: str\n",
    "    parents: tuple\n",
    "    values: tuple\n",
    "    probability_model: Optional[np.ndarray] = field(default=None, compare=False, repr=False)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.name)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Node):\n",
    "            return self.name == other.name\n",
    "        return False\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.name}: [{\", \".join(self.values)}]'\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, nodes: Set[Node] | None = None):\n",
    "        self.nodes: Set[Node] = nodes or set()\n",
    "        self.parents: Dict[Node, Set[Node]] = defaultdict(set)   # child -> {parents}\n",
    "        self.children: Dict[Node, Set[Node]] = defaultdict(set)  # parent -> {children}\n",
    "        self.by_name: Dict[str, Node] = {}\n",
    "\n",
    "    def add_node(self, node: Node):\n",
    "        if node.name not in self.by_name:\n",
    "            self.by_name[node.name] = node\n",
    "            self.nodes.add(node)\n",
    "\n",
    "    def add_edge(self, parent_name: str, child_name: str):\n",
    "        parent = self.by_name[parent_name]\n",
    "        child = self.by_name[child_name]\n",
    "        self.parents[child].add(parent)\n",
    "        self.children[parent].add(child)\n",
    "\n",
    "    def markov_blanket(self, node: Node) -> Set[Node]:\n",
    "        blanket: Set[Node] = self.parents.get(node, set())\n",
    "        children = self.children.get(node, set())\n",
    "        blanket.update(children)\n",
    "        # children's parents excluding self\n",
    "        for child in children:\n",
    "            blanket.update(parent for parent in self.parents.get(child, set()) if parent is not node)\n",
    "        blanket.discard(node)\n",
    "        return blanket\n",
    "\n",
    "    def degree(self, node: Node) -> int:\n",
    "        return len(self.parents[node])+len(self.children[node])\n",
    "\n",
    "    def topological_order(self):\n",
    "        degree_in_context = {node:len(self.parents[node]) for node in self.nodes}\n",
    "        node_queue = deque(sorted(self.nodes, key=lambda node: degree_in_context[node]))\n",
    "\n",
    "        output = []\n",
    "        while node_queue:\n",
    "            current_node = node_queue.pop()\n",
    "            if degree_in_context[current_node] == 0:\n",
    "                output.append(current_node)\n",
    "                for child in self.children[current_node]:\n",
    "                    degree_in_context[child] -= 1\n",
    "            else:\n",
    "                node_queue.appendleft(current_node)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join(str(n) for n in sorted(self.nodes, key=lambda n: n.name))\n",
    "\n",
    "class InputReader:\n",
    "    def __init__(self):\n",
    "        reader = BIFReader(NETWORK_NAME)\n",
    "        model = reader.get_model()\n",
    "        states = reader.get_states()\n",
    "        net = Network()\n",
    "\n",
    "\n",
    "        for variable in model.nodes():\n",
    "            cpd = model.get_cpds(variable)\n",
    "\n",
    "            parents = tuple(cpd.get_evidence() or ())\n",
    "\n",
    "            child_name = str(variable)  # CHANGED\n",
    "            child_card = len(states[child_name])  # CHANGED\n",
    "            parent_cards = [len(states[p]) for p in parents]  # (parents already strings from get_evidence)\n",
    "\n",
    "            pm_nd = np.array(cpd.values, dtype=float).reshape(\n",
    "                (child_card, *parent_cards),\n",
    "                order=\"F\"\n",
    "            )\n",
    "\n",
    "            net.add_node(Node(\n",
    "                name=child_name,                  # CHANGED\n",
    "                parents=parents,\n",
    "                values=tuple(states[child_name]), # CHANGED\n",
    "                probability_model=pm_nd\n",
    "            ))\n",
    "\n",
    "        # 2) add edges\n",
    "        for child in model.nodes():\n",
    "            cpd = model.get_cpds(child)\n",
    "            for parent in (cpd.get_evidence() or []):\n",
    "                net.add_edge(str(parent), str(child))\n",
    "\n",
    "        self.network = net\n",
    "\n",
    "        # 3) parse EVIDENCE string\n",
    "        self.parsed_evidence = {}\n",
    "        if EVIDENCE:\n",
    "            for statement in EVIDENCE.split(';'):\n",
    "                statement = statement.strip()     # CHANGED: ignore empty pieces\n",
    "                if not statement:                 # CHANGED\n",
    "                    continue                      # CHANGED\n",
    "                i = statement.find('=')\n",
    "                key = statement[:i].strip()\n",
    "                value = statement[i+1:].strip()\n",
    "                if key not in self.network.by_name:\n",
    "                    raise Exception(\"Invalid Evidence \" + key)\n",
    "                self.parsed_evidence[key] = value\n",
    "\n",
    "        # 4) parse report\n",
    "        self.parsed_report = []\n",
    "        self.parsed_report = REPORT\n",
    "        self.parsed_report = self.parsed_report[1:] if self.parsed_report.startswith(\"[\") else self.parsed_report\n",
    "        self.parsed_report = self.parsed_report[:-1] if self.parsed_report.endswith(\"]\") else self.parsed_report\n",
    "        self.parsed_report = [s.strip() for s in self.parsed_report.split(REPORT_DELIM)]\n",
    "\n",
    "class Factor:\n",
    "    def __init__(self, variables, values):\n",
    "\n",
    "        self.variables = list(variables)\n",
    "        self.values = np.array(values, dtype=np.float32)\n",
    "        assert self.values.ndim == len(self.variables), \\\n",
    "            f\"values.ndim ({self.values.ndim}) must equal len(variables) ({len(self.variables)}).\"\n",
    "\n",
    "    def reorder(self, vars_order):\n",
    "        \"\"\"Reorder axes to match vars_order.\"\"\"\n",
    "        if set(vars_order) != set(self.variables):\n",
    "            raise ValueError(\n",
    "                f\"reorder mismatch. have={self.variables}, want={vars_order}\"\n",
    "            )\n",
    "        source = list(range(len(self.variables)))\n",
    "        destination = [vars_order.index(v) for v in self.variables]\n",
    "        self.values = np.moveaxis(self.values, source, destination)\n",
    "        self.variables = list(vars_order)\n",
    "        return self\n",
    "\n",
    "    def restrict(self, var, value_index):\n",
    "        if var not in self.variables:\n",
    "            return self\n",
    "        ax = self.variables.index(var)\n",
    "        self.values = np.take(self.values, indices=value_index, axis=ax)\n",
    "        self.variables.pop(ax)\n",
    "        return self\n",
    "\n",
    "    def _align_to(self, all_vars):\n",
    "        \"\"\"Return a view to order all_vars.\"\"\"\n",
    "        dest_axes = [all_vars.index(v) for v in self.variables]\n",
    "\n",
    "        arr = self.values\n",
    "        need = len(all_vars) - arr.ndim\n",
    "        if need > 0:\n",
    "            arr = arr.reshape(arr.shape + (1,) * need)\n",
    "\n",
    "        arr = np.moveaxis(arr, list(range(len(self.variables))), dest_axes)\n",
    "        return arr\n",
    "\n",
    "    def multiply(self, other):\n",
    "        \"\"\"Pointwise multiply after aligning axes by variable name.\"\"\"\n",
    "        all_vars = list(dict.fromkeys(self.variables + other.variables))\n",
    "        A = self._align_to(all_vars)\n",
    "        B = other._align_to(all_vars)\n",
    "        prod = A * B  # numpy handles missing vars\n",
    "        return Factor(all_vars, prod)\n",
    "\n",
    "    def sum_out(self, var):\n",
    "        if var not in self.variables:\n",
    "            return self\n",
    "        ax = self.variables.index(var)\n",
    "        self.values = self.values.sum(axis=ax)\n",
    "        self.variables.pop(ax)\n",
    "        return self\n",
    "\n",
    "    def normalize(self):\n",
    "        Z = self.values.sum()\n",
    "        if Z != 0:\n",
    "            self.values = self.values / Z\n",
    "        # else: leave as-is\n",
    "        return self\n",
    "\n",
    "class VESolver:\n",
    "    @staticmethod\n",
    "    def _value_index(node, label: str) -> int:\n",
    "        try:\n",
    "            return node.values.index(label)\n",
    "        except ValueError:\n",
    "            lower_vals = [s.lower() for s in node.values]\n",
    "            try:\n",
    "                return lower_vals.index(label.lower())\n",
    "            except ValueError:\n",
    "                allowed = \", \".join(node.values)\n",
    "                raise ValueError(\n",
    "                    f\"Value {label!r} invalid for {node.name}. Allowed: [{allowed}]\"\n",
    "                )\n",
    "\n",
    "    def _factor_from_node(self, network, node) -> \"Factor\":\n",
    "        \"\"\"\n",
    "        Assumes CPD stored as shape (child, *parents)\n",
    "        \"\"\"\n",
    "        child = node.name\n",
    "        parents = list(node.parents or ())\n",
    "        variables = [child] + parents\n",
    "        vals = np.array(node.probability_model, dtype=float, copy=True)\n",
    "        return Factor(variables, vals)\n",
    "\n",
    "    def _build_factors(self, network) -> list[\"Factor\"]:\n",
    "        return [self._factor_from_node(network, n) for n in network.nodes]\n",
    "\n",
    "    def _ancestors_of(self, network, vars_set: set[str]) -> set[str]:\n",
    "        \"\"\"\n",
    "        Return vars_set ∪ all their (recursive) parents\n",
    "        \"\"\"\n",
    "        keep = set(vars_set)\n",
    "        stack = list(vars_set)\n",
    "        while stack:\n",
    "            v = stack.pop()\n",
    "            node = network.by_name[v]\n",
    "            for p in (node.parents or ()):\n",
    "                if p not in keep:\n",
    "                    keep.add(p)\n",
    "                    stack.append(p)\n",
    "        return keep\n",
    "\n",
    "    def _build_factors_for(self, network, keep_vars: set[str]) -> list[\"Factor\"]:\n",
    "        \"\"\"Only build factors for nodes whose variable is in keep_vars.\"\"\"\n",
    "\n",
    "        return [\n",
    "            self._factor_from_node(network, n)\n",
    "            for n in network.nodes\n",
    "            if n.name in keep_vars\n",
    "        ]\n",
    "\n",
    "    def solve(self, network, query: List[str] | str, evidence: dict[str, str]):\n",
    "        if isinstance(query, str):\n",
    "            query = [query]\n",
    "\n",
    "        if query[0] in evidence:\n",
    "            node = network.by_name[query[0]]\n",
    "            num_values = len(node.values)\n",
    "            return Factor([query[0]], [-1]*num_values)\n",
    "\n",
    "        # Map evidence labels, indices (case-insensitive, validated)\n",
    "        evidence_node_states = {}\n",
    "        for variable, label in evidence.items():\n",
    "            node = network.by_name[variable]\n",
    "            evidence_node_states[variable] = self._value_index(node, label)\n",
    "\n",
    "\n",
    "        frontier = set(query) | set(evidence.keys())\n",
    "        keep_vars = self._ancestors_of(network, frontier) | frontier\n",
    "\n",
    "\n",
    "        # Build and restrict factors (only ancestors kept)\n",
    "        factors = self._build_factors_for(network, keep_vars)\n",
    "        for variable, idx in evidence_node_states.items():\n",
    "            for factor in factors:\n",
    "                factor.restrict(variable, idx)\n",
    "\n",
    "        # Elimination order: topological over kept vars, drop evidence & query\n",
    "        elim_order = [\n",
    "            node.name for node in network.topological_order()\n",
    "            if node.name in keep_vars\n",
    "            and node.name not in evidence\n",
    "            and node.name not in query\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Variable elimination\n",
    "        for elim in elim_order:\n",
    "            bucket = [f for f in factors if elim in f.variables]\n",
    "            if not bucket:\n",
    "                continue\n",
    "            new_factor = Factor(bucket[0].variables[:],\n",
    "                                np.array(bucket[0].values, dtype=np.float32, copy=True))\n",
    "            for f in bucket[1:]:\n",
    "                new_factor = new_factor.multiply(f)\n",
    "            new_factor.sum_out(elim)\n",
    "            factors = [f for f in factors if f not in bucket] + [new_factor]\n",
    "\n",
    "\n",
    "        # Multiply remaining factors\n",
    "        if not factors:\n",
    "            raise RuntimeError(\"No factors remain after elimination.\")\n",
    "        result = factors[0]\n",
    "        for f in factors[1:]:\n",
    "            result = result.multiply(f)\n",
    "\n",
    "        # Sum out everything not in query\n",
    "        for v in list(result.variables):\n",
    "            if v not in query:\n",
    "                result.sum_out(v)\n",
    "\n",
    "        # Reorder + normalize\n",
    "        result.reorder(query).normalize()\n",
    "        return result\n",
    "\n",
    "class GibbsSolver:\n",
    "    \"\"\"\n",
    "    - network.by_name: dict[str, Node]\n",
    "    - network.parents: dict[Node, set[Node]]\n",
    "    - network.children: dict[Node, set[Node]]\n",
    "    - Node: name(str), parents(tuple[str]), values(tuple[str]), probability_model(np.ndarray)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, iterations=2000, burn_in=500, thin=1, seed=None, verbose=False, eps: float = 1e-4):  # ADDED: eps\n",
    "        assert iterations > 0 and 0 <= burn_in < iterations\n",
    "        assert thin >= 1\n",
    "        self.iterations = iterations\n",
    "        self.burn_in = burn_in\n",
    "        self.thin = thin\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.verbose = verbose\n",
    "        self.eps = eps  # ADDED\n",
    "\n",
    "    def solve(self, network, report_var: str, evidence: dict[str, str]):\n",
    "        if report_var in evidence:\n",
    "            node = network.by_name[report_var]\n",
    "            num_values = len(node.values)\n",
    "            return Factor([report_var], [-1] * num_values)\n",
    "\n",
    "        nodes_by_name = network.by_name\n",
    "        report_node = nodes_by_name[report_var]\n",
    "\n",
    "        state = {}\n",
    "        for name, node in nodes_by_name.items():\n",
    "            if name in evidence:\n",
    "                state[name] = evidence[name]\n",
    "            else:\n",
    "                state[name] = self.rng.choice(node.values)\n",
    "\n",
    "        non_evidence_nodes = [nodes_by_name[n] for n in nodes_by_name if n not in evidence]\n",
    "\n",
    "        tally = Counter()\n",
    "        kept = 0\n",
    "        try:\n",
    "            from tqdm import tqdm\n",
    "            it_range = tqdm(range(self.iterations), desc=f\"Gibbs sampling: {report_var}\")\n",
    "        except Exception:\n",
    "            it_range = range(self.iterations)\n",
    "\n",
    "        for it in it_range:\n",
    "            self.rng.shuffle(non_evidence_nodes)  # ADDED\n",
    "\n",
    "            for X_node in non_evidence_nodes:\n",
    "                X_name = X_node.name\n",
    "                x_vals = X_node.values\n",
    "\n",
    "                # --- CHANGED: compute in log-space to avoid underflow ---\n",
    "                log_weights = np.empty(len(x_vals), dtype=float)\n",
    "\n",
    "                for i, x in enumerate(x_vals):\n",
    "                    old_val = state[X_name]\n",
    "                    state[X_name] = x\n",
    "\n",
    "                    # log p(X=x | Pa(X))\n",
    "                    px = max(self._cpd_prob(X_node, state, nodes_by_name), self.eps)  # ADDED guard\n",
    "                    lw = np.log(px)\n",
    "\n",
    "                    # sum_Y log p(Y=y | Pa(Y))\n",
    "                    for Y_node in network.children.get(X_node, ()):\n",
    "                        py = max(self._cpd_prob(Y_node, state, nodes_by_name), self.eps)  # ADDED guard\n",
    "                        lw += np.log(py)\n",
    "\n",
    "                    log_weights[i] = lw\n",
    "                    state[X_name] = old_val\n",
    "\n",
    "                # softmax for normalization (log-sum-exp)  --- CHANGED\n",
    "                m = np.max(log_weights)\n",
    "                stable = np.exp(log_weights - m)\n",
    "                Z = stable.sum()\n",
    "                if not np.isfinite(Z) or Z <= 0:\n",
    "                    probs = np.full(len(x_vals), 1.0 / len(x_vals))\n",
    "                else:\n",
    "                    probs = stable / Z\n",
    "\n",
    "                state[X_name] = self.rng.choice(x_vals, p=probs)\n",
    "\n",
    "            if it >= self.burn_in and ((it - self.burn_in) % self.thin == 0):\n",
    "                tally[state[report_var]] += 1\n",
    "                kept += 1\n",
    "\n",
    "        if kept == 0:\n",
    "            tally[state[report_var]] += 1\n",
    "            kept = 1\n",
    "\n",
    "        report_values = report_node.values\n",
    "        counts = np.array([tally[v] for v in report_values], dtype=float)\n",
    "        probs = counts / counts.sum()\n",
    "\n",
    "        return Factor([report_var], np.array(probs, dtype=np.float32))\n",
    "\n",
    "    def _cpd_prob(self, node, state, nodes_by_name):\n",
    "        \"\"\"\n",
    "        ADDED: epsilon-smooth the CPD column \n",
    "        \"\"\"\n",
    "        pm = node.probability_model\n",
    "        par_names = node.parents or ()\n",
    "\n",
    "        x_label = state[node.name]\n",
    "        try:\n",
    "            x_idx = node.values.index(x_label)\n",
    "        except ValueError:\n",
    "            return 0.0\n",
    "\n",
    "        par_indices = []\n",
    "        for p in par_names:\n",
    "            dom = nodes_by_name[p].values\n",
    "            pv = state[p]\n",
    "            try:\n",
    "                par_indices.append(dom.index(pv))\n",
    "            except ValueError:\n",
    "                return 0.0\n",
    "\n",
    "        try:\n",
    "            if pm.ndim == 1 + len(par_names):\n",
    "                col_vec = pm[(slice(None), *par_indices)].astype(float, copy=True)\n",
    "            elif pm.ndim == 2:\n",
    "                if len(par_names) == 0:\n",
    "                    col_vec = (pm[:, 0] if pm.shape[1] == 1 else pm[:, :]).astype(float, copy=True)\n",
    "                    if col_vec.ndim > 1:\n",
    "                        col_vec = col_vec[:, 0]\n",
    "                else:\n",
    "                    col = 0\n",
    "                    bases = [len(nodes_by_name[p].values) for p in par_names]\n",
    "                    for i, b in zip(reversed(par_indices), reversed(bases)):\n",
    "                        col = col * b + i\n",
    "                    col_vec = pm[:, col].astype(float, copy=True)\n",
    "            else:\n",
    "                return 0.0\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "        # ADDED: epsilon smoothing and renormalization \n",
    "        eps = float(getattr(self, \"eps\", 1e-6))\n",
    "        col_vec = np.maximum(col_vec, eps)\n",
    "        s = col_vec.sum()\n",
    "        if not np.isfinite(s) or s <= 0:\n",
    "            return 1.0 / len(col_vec)\n",
    "        col_vec /= s\n",
    "\n",
    "        return float(col_vec[x_idx])\n",
    "\n",
    "class OutputWriter:\n",
    "    def __init__(self, output: List[List[Union[str, int]]]):\n",
    "        os.makedirs(\"outputs\", exist_ok=True)\n",
    "        name = os.path.splitext(os.path.basename(NETWORK_NAME))[0]\n",
    "        file_path = f\"outputs/group{GROUP_ID}_{ALGORITHM}_{name}_{EVIDENCE_LEVEL}.csv\"\n",
    "        print(f\"Writing to {file_path}\")\n",
    "\n",
    "        with open(file_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(output)\n",
    "\n",
    "class Driver:\n",
    "    def __init__(self):\n",
    "        self.reader = InputReader()\n",
    "        match(ALGORITHM.lower()):\n",
    "            case \"ve\":\n",
    "                self.solver = VESolver()\n",
    "            case \"gibbs\":\n",
    "                self.solver = GibbsSolver(iterations=20_000, burn_in=5_000, thin=5, seed=GROUP_ID, eps=1e-4)\n",
    "\n",
    "            case _:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        marginals = [self.solver.solve(self.reader.network,report,self.reader.parsed_evidence) for report in self.reader.parsed_report]\n",
    "        output = []\n",
    "        for f in marginals:\n",
    "            var = f.variables[0]\n",
    "            output.append([var] + list(self.reader.network.by_name[var].values))\n",
    "            if all([i == -1 for i in f.values]):\n",
    "                output.append(['x']*len(f.values))\n",
    "            else:\n",
    "                output.append([f\"{n:.2f}\" for n in f.values])\n",
    "\n",
    "\n",
    "        for line in output:\n",
    "            print(\", \".join(line))\n",
    "\n",
    "        OutputWriter(output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    driver = Driver()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI446 (Py3.11)",
   "language": "python",
   "name": "csci446-p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
