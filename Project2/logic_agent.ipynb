{
 "cells": [
  {
   "cell_type": "code",
   "id": "952df08cada322c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T01:42:28.678308Z",
     "start_time": "2025-10-09T01:12:20.172825Z"
    }
   },
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import io\n",
    "import sys\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum, auto\n",
    "from typing import List, Union, Dict, Tuple, Optional, Any, TypeAlias, Iterable, Callable, FrozenSet\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "#Parameters\n",
    "filepath = \"Caves/easy/path_e3.txt\"\n",
    "\n",
    "# <editor-fold desc=\"Semantics and structure for FOL\">\n",
    "\n",
    "class TokenType(Enum):\n",
    "    XOR = auto()\n",
    "    OR = auto()\n",
    "    AND = auto()\n",
    "    NOT = auto()\n",
    "    ALL = auto()\n",
    "    ANY = auto()\n",
    "    IMPLIES = auto()\n",
    "    IFF = auto()\n",
    "    IN = auto()\n",
    "\n",
    "    TRUE = auto()\n",
    "    FALSE = auto()\n",
    "    IDENT = auto()\n",
    "\n",
    "    LPAREN = auto()\n",
    "    RPAREN = auto()\n",
    "    COMMA  = auto()\n",
    "\n",
    "    def __str__(self): return self.name\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Token:\n",
    "    type: TokenType\n",
    "    name: Optional[str] = None  # used for variables quantifiers and predicates\n",
    "\n",
    "class Lexer:\n",
    "    _whitespace = re.compile(r\"\\s+\")\n",
    "    _identifier = re.compile(r\"[A-Za-z_][A-Za-z0-9_]*\")\n",
    "\n",
    "    SYMBOLS = [\n",
    "        (\"(\", TokenType.LPAREN),\n",
    "        (\")\", TokenType.RPAREN),\n",
    "        (\",\", TokenType.COMMA),\n",
    "    ]\n",
    "\n",
    "    # keywords, case-sensitive\n",
    "    KEYWORDS = {\n",
    "        \"TRUE\": TokenType.TRUE,\n",
    "        \"FALSE\": TokenType.FALSE,\n",
    "\n",
    "        \"AND\": TokenType.AND,\n",
    "        \"OR\": TokenType.OR,\n",
    "        \"XOR\": TokenType.XOR,\n",
    "        \"NOT\": TokenType.NOT,\n",
    "\n",
    "        \"IMPLIES\": TokenType.IMPLIES,\n",
    "        \"IFF\": TokenType.IFF,\n",
    "\n",
    "        \"ALL\": TokenType.ALL,\n",
    "        \"ANY\": TokenType.ANY,\n",
    "        \"IN\": TokenType.IN,\n",
    "    }\n",
    "\n",
    "    def tokenize(self, text: str) -> List[Token]:\n",
    "        tokens: List[Token] = []\n",
    "        i = 0\n",
    "        length = len(text)\n",
    "\n",
    "        while i < length:\n",
    "            # skip whitespace\n",
    "            current = self._whitespace.match(text, i)\n",
    "            if current:\n",
    "                i = current.end()\n",
    "                if i >= length:\n",
    "                    break\n",
    "\n",
    "            matched_symbol = False\n",
    "            for symbol, token_type in self.SYMBOLS:\n",
    "                if text.startswith(symbol, i):\n",
    "                    tokens.append(Token(token_type))\n",
    "                    i += len(symbol)\n",
    "                    matched_symbol = True\n",
    "                    break\n",
    "            if matched_symbol:\n",
    "                continue\n",
    "\n",
    "            # Identifier / keyword\n",
    "            current = self._identifier.match(text, i)\n",
    "            if current:\n",
    "                lex = current.group(0)\n",
    "                i = current.end()\n",
    "\n",
    "                # Keyword\n",
    "                token_type = self.KEYWORDS.get(lex)\n",
    "                if token_type is not None:\n",
    "                    tokens.append(Token(token_type))\n",
    "                else:\n",
    "                    tokens.append(Token(TokenType.IDENT, name=lex))\n",
    "                continue\n",
    "\n",
    "            # fallback\n",
    "            raise ValueError(f\"Unexpected character at {i}: {repr(text[i])}\")\n",
    "\n",
    "        return tokens\n",
    "\n",
    "# Terms\n",
    "class LogicTerminal(Enum):\n",
    "    U = auto()  # Unknown\n",
    "    F = auto()  # False\n",
    "    T = auto()  # True\n",
    "\n",
    "    def __str__(self):\n",
    "        if self is LogicTerminal.U: return \"Unknown\"\n",
    "        if self is LogicTerminal.F: return \"False\"\n",
    "        if self is LogicTerminal.T: return \"True\"\n",
    "        return \"NULL\"\n",
    "\n",
    "    def __bool__(self): return self is LogicTerminal.T\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Variable:\n",
    "    name: Any\n",
    "    def __str__(self) -> str: return str(self.name)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Constant:\n",
    "    value: Any = LogicTerminal.U\n",
    "    def __str__(self) -> str: return str(self.value)\n",
    "    def __eq__(self, other: Any) -> bool:\n",
    "        if not isinstance(other, Constant): return False\n",
    "        return self.value == other.value\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Predicate:\n",
    "    name: str\n",
    "    args: Tuple[Any, ...] = field(default_factory=tuple)  # must be immutable for hashing\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.name}({', '.join(map(str, self.args))})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Predicate): return False\n",
    "        return self.name == other.name and self.args == other.args\n",
    "\n",
    "\n",
    "class LogicOperator(Enum):\n",
    "    XOR = auto()\n",
    "    AND = auto()\n",
    "    OR = auto()\n",
    "    IFF = auto()\n",
    "    IMPLIES = auto()\n",
    "\n",
    "    def __str__(self): return self.name\n",
    "\n",
    "class Quantifier(Enum):\n",
    "    ANY = auto()\n",
    "    ALL = auto()\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Not:\n",
    "    child: Any\n",
    "\n",
    "    def __str__(self): return \"Not \"+str(self.child)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Operator:\n",
    "    nodeType: LogicOperator\n",
    "    children: List[Any] = field(default_factory=list)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class QuantifierExpression:\n",
    "    quantifier: Quantifier\n",
    "    variables: Tuple[Variable, ...]\n",
    "    domain: Any\n",
    "    expression: 'Expression'\n",
    "\n",
    "SimpleTerm: TypeAlias = Union[Variable, Constant, LogicTerminal, None]\n",
    "Expression: TypeAlias = Union[Predicate, Not, Operator, QuantifierExpression, Constant, Variable]\n",
    "Term = Union[SimpleTerm, Expression]\n",
    "\n",
    "\n",
    "class Parser:\n",
    "    def __init__(self):\n",
    "        self.expression: List[Token] = []\n",
    "        self.parse_index = 0\n",
    "\n",
    "    def __call__(self, arg: Union[str,List[Token]]):\n",
    "        if isinstance(arg, str):\n",
    "            lex = Lexer()\n",
    "            self.expression = lex.tokenize(arg)\n",
    "        elif isinstance(arg, list):\n",
    "            self.expression = arg\n",
    "        return self.parse(self.expression)\n",
    "\n",
    "    def parse(self, tokens: List[Token]) -> Term:\n",
    "        self.expression = tokens\n",
    "        self.parse_index = 0\n",
    "        return self.parse_expression()\n",
    "\n",
    "    def peek(self, k=0) -> Optional[Token]:\n",
    "        i = self.parse_index + k\n",
    "        return self.expression[i] if 0 <= i < len(self.expression) else None\n",
    "\n",
    "    def peek_is(self, t: TokenType) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if next token exists and is of a certain type\n",
    "        \"\"\"\n",
    "        tok = self.peek()\n",
    "        return tok is not None and tok.type is t\n",
    "\n",
    "    def eat(self) -> Optional[Token]:\n",
    "        tok = self.peek()\n",
    "        if tok is not None:\n",
    "            self.parse_index += 1\n",
    "        return tok\n",
    "\n",
    "    def expect(self, token_type: TokenType) -> Token:\n",
    "        tok = self.eat()\n",
    "        if tok is None or tok.type is not token_type:\n",
    "            raise ValueError(f\"Expected {token_type}, got {tok}\")\n",
    "        return tok\n",
    "\n",
    "    def parse_expression(self) -> Term:\n",
    "        node = self._parse_iff()\n",
    "        if self.peek() is not None:\n",
    "            raise ValueError(f\"Expression not empty after parsing\")\n",
    "        return node\n",
    "\n",
    "    def _parse_iff(self) -> Term:\n",
    "        node = self._parse_implies()\n",
    "        while self.peek_is(TokenType.IFF):\n",
    "            self.eat()\n",
    "            rhs = self._parse_implies()\n",
    "            node = self._reduce_iff(node, rhs)\n",
    "        return node\n",
    "\n",
    "    def _parse_implies(self) -> Term:\n",
    "        left = self._parse_xor()\n",
    "        if self.peek_is(TokenType.IMPLIES):\n",
    "            self.eat()\n",
    "            right = self._parse_implies()  # right-assoc\n",
    "            return self.reduce_implies(left, right)\n",
    "        return left\n",
    "\n",
    "    def _parse_xor(self) -> Term:\n",
    "        node = self._parse_or()\n",
    "        while self.peek_is(TokenType.XOR):\n",
    "            self.eat()\n",
    "            rhs = self._parse_or()\n",
    "            node = Operator(nodeType=LogicOperator.XOR, children=[node, rhs])\n",
    "        return node\n",
    "\n",
    "    def _parse_or(self) -> Term:\n",
    "        node = self._parse_and()\n",
    "        while self.peek_is(TokenType.OR):\n",
    "            self.eat()\n",
    "            rhs = self._parse_and()\n",
    "            node = Operator(nodeType=LogicOperator.OR, children=[node, rhs])\n",
    "        return node\n",
    "\n",
    "    def _parse_and(self) -> Term:\n",
    "        node = self._parse_not()\n",
    "        while self.peek_is(TokenType.AND):\n",
    "            self.eat()\n",
    "            rhs = self._parse_not()\n",
    "            node = Operator(nodeType=LogicOperator.AND, children=[node, rhs])\n",
    "        return node\n",
    "\n",
    "    def _parse_not(self) -> Term:\n",
    "        if self.peek_is(TokenType.NOT):\n",
    "            self.eat()\n",
    "            return Not(child=self._parse_not())\n",
    "        return self._parse_atom()\n",
    "\n",
    "    def _parse_atom(self) -> Term:\n",
    "        tok = self.peek()\n",
    "        if tok is None:\n",
    "            raise ValueError(\"Unexpected end of expression\")\n",
    "\n",
    "        # booleans\n",
    "        if tok.type is TokenType.TRUE:\n",
    "            self.eat()\n",
    "            return Constant(LogicTerminal.T)\n",
    "        if tok.type is TokenType.FALSE:\n",
    "            self.eat()\n",
    "            return Constant(LogicTerminal.F)\n",
    "\n",
    "        #'(' expr ')'\n",
    "        if tok.type is TokenType.LPAREN:\n",
    "            self.eat()\n",
    "            node = self._parse_iff()\n",
    "            self.expect(TokenType.RPAREN)\n",
    "            return node\n",
    "\n",
    "        # Quantifier (ALL/ANY)\n",
    "        if self.peek_is(TokenType.ALL) or self.peek_is(TokenType.ANY):\n",
    "            return self._parse_quantifier()\n",
    "\n",
    "        # Predicate or variable: IDENT [ '(' args ')' ]\n",
    "        if tok.type is TokenType.IDENT:\n",
    "            ident = self.eat()\n",
    "            name = ident.name\n",
    "\n",
    "            if self.peek_is(TokenType.LPAREN):\n",
    "                self.eat()\n",
    "                args: List[Term] = []\n",
    "                if not self.peek_is(TokenType.RPAREN):\n",
    "                    while True:\n",
    "                        args.append(self._parse_term())\n",
    "                        if self.peek_is(TokenType.COMMA):\n",
    "                            self.eat()\n",
    "                            continue\n",
    "                        break\n",
    "                self.expect(TokenType.RPAREN)\n",
    "                return Predicate(name=name, args=tuple(args))\n",
    "\n",
    "            # variable\n",
    "            return Variable(name=name)\n",
    "\n",
    "        raise ValueError(f\"Unexpected token in atom: {tok}\")\n",
    "\n",
    "    def _parse_term(self) -> Term:\n",
    "        tok = self.peek()\n",
    "        if tok is None:\n",
    "            raise ValueError(\"Unexpected end of arguments\")\n",
    "        if tok.type is TokenType.TRUE:\n",
    "            self.eat()\n",
    "            return Constant(LogicTerminal.T)\n",
    "        if tok.type is TokenType.FALSE:\n",
    "            self.eat()\n",
    "            return Constant(LogicTerminal.F)\n",
    "        if tok.type is TokenType.IDENT:\n",
    "            return Variable(name=self.eat().name)\n",
    "        if tok.type is TokenType.LPAREN:\n",
    "            raise ValueError(\"Nested predicate arguments not supported\")\n",
    "        raise ValueError(f\"Invalid token: {tok}\")\n",
    "\n",
    "    def _parse_quantifier(self) -> Term:\n",
    "        quantifier_token = self.eat()\n",
    "        quantifier = Quantifier.ALL if quantifier_token.type is TokenType.ALL else Quantifier.ANY\n",
    "\n",
    "        # Variables: IDENT or '(' IDENT (',' IDENT)* ')'\n",
    "        variables: List[Variable] = []\n",
    "        if self.peek_is(TokenType.LPAREN):\n",
    "            self.eat()\n",
    "            while True:\n",
    "                ident = self.expect(TokenType.IDENT)\n",
    "                variables.append(Variable(name=ident.name))\n",
    "                if self.peek_is(TokenType.COMMA):\n",
    "                    self.eat()\n",
    "                    continue\n",
    "                break\n",
    "            self.expect(TokenType.RPAREN)\n",
    "        else:\n",
    "            ident = self.expect(TokenType.IDENT)\n",
    "            variables.append(Variable(name=ident.name))\n",
    "\n",
    "        # IN domain\n",
    "        self.expect(TokenType.IN)\n",
    "        domain_token = self.expect(TokenType.IDENT)\n",
    "        domain = domain_token.name\n",
    "\n",
    "        if self.peek_is(TokenType.LPAREN):\n",
    "            self.eat()\n",
    "            body = self._parse_iff()\n",
    "            self.expect(TokenType.RPAREN)\n",
    "        else:\n",
    "            body = self._parse_iff()\n",
    "\n",
    "        return QuantifierExpression(quantifier=quantifier, variables=tuple(variables), domain=domain, expression=body)\n",
    "\n",
    "    # Reductions for IMPLIES/IFF\n",
    "    @staticmethod\n",
    "    def reduce_implies(p: Term, q: Term) -> Term:\n",
    "        # p -> q  ==  (!p) OR q\n",
    "        return Operator(nodeType=LogicOperator.OR, children=[Not(p), q])\n",
    "\n",
    "    def _reduce_iff(self, p: Term, q: Term) -> Term:\n",
    "        # p <-> q  ==  (p -> q) AND (q -> p)\n",
    "        return Operator(\n",
    "            nodeType=LogicOperator.AND,\n",
    "            children=[self.reduce_implies(p, q), self.reduce_implies(q, p)]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def pretty_print(node: Term, indent: str = \"\", is_last: bool = True):\n",
    "        branch = \"\\\\-- \" if is_last else \"| \"\n",
    "        next_indent = indent + (\"    \" if is_last else \"|   \")\n",
    "\n",
    "        # Operator\n",
    "        if isinstance(node, Operator):\n",
    "            label = str(node.nodeType.name)\n",
    "            print(indent + branch + label)\n",
    "            for i, child in enumerate(node.children):\n",
    "                Parser.pretty_print(child, next_indent, i == len(node.children) - 1)\n",
    "            return\n",
    "\n",
    "        # NOT\n",
    "        if isinstance(node, Not):\n",
    "            print(indent + branch + \"NOT\")\n",
    "            Parser.pretty_print(node.child, next_indent, True)\n",
    "            return\n",
    "\n",
    "        # Predicate\n",
    "        if isinstance(node, Predicate):\n",
    "            print(indent + branch + str(node))\n",
    "            return\n",
    "\n",
    "        # Constant (LogicTerminal)\n",
    "        if isinstance(node, Constant):\n",
    "            print(indent + branch + str(node.value))\n",
    "            return\n",
    "\n",
    "        # Variable\n",
    "        if isinstance(node, Variable):\n",
    "            print(indent + branch + f\"Variable({node.name})\")\n",
    "            return\n",
    "\n",
    "        # Quantifier\n",
    "        if isinstance(node, QuantifierExpression):\n",
    "            quantifier_name = str(node.quantifier.name)\n",
    "            variables_str = \", \".join(v.name for v in node.variables)\n",
    "            domain_str = node.domain\n",
    "            label = f\"{quantifier_name} {variables_str} IN {domain_str}\"\n",
    "            print(indent + branch + label)\n",
    "            Parser.pretty_print(node.expression, next_indent, True)\n",
    "            return\n",
    "\n",
    "        # LogicTerminal\n",
    "        if isinstance(node, LogicTerminal):\n",
    "            print(indent + branch + str(node))\n",
    "            return\n",
    "\n",
    "        # Fallback\n",
    "        print(indent + branch + f\"{node}\")\n",
    "\n",
    "\n",
    "\n",
    "class ExpressionEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate an AST\n",
    "    - For propositional variables: look up in variable_environment (by variable name)\n",
    "    - For predicates consult predicate_table\n",
    "    - For quantifiers need domains\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Term,\n",
    "        variable_environment: Optional[Dict[str, LogicTerminal]] = None,\n",
    "        predicate_table: Optional[Dict[Tuple[str, Tuple[Any, ...]], LogicTerminal]] = None,\n",
    "        domains: Optional[Dict[str, Union[Iterable,Callable]]] = None,\n",
    "    ):\n",
    "        self.root = root\n",
    "        self.variable_environment = variable_environment or {}\n",
    "        self.predicate_table = predicate_table or {}\n",
    "        self.domains = domains or {}\n",
    "\n",
    "        # current variable bindings\n",
    "        self.bindings: Dict[str, Any] = {}\n",
    "\n",
    "        self.evaluation: LogicTerminal = self.eval(root)\n",
    "\n",
    "    def _resolve_domain(self, name: str) -> Iterable[Any]:\n",
    "        if name not in self.domains:\n",
    "            raise ValueError(f\"Domain '{name}' not provided.\")\n",
    "        domain = self.domains[name]\n",
    "        # If callable, pass current bindings so it can depend on bound vars\n",
    "        return domain(self.bindings) if callable(domain) else domain\n",
    "\n",
    "    @staticmethod\n",
    "    def implies(p: Term, q: Term) -> Term:\n",
    "        return Operator(LogicOperator.OR, [Not(p), q])\n",
    "\n",
    "    @staticmethod\n",
    "    def equivalence(p: Term, q: Term) -> LogicTerminal:\n",
    "        \"\"\"\n",
    "        Evaluate (p -> q) ∧ (q -> p).\n",
    "        \"\"\"\n",
    "        new_expression_tree = Operator(LogicOperator.AND, [ExpressionEvaluator.implies(p, q), ExpressionEvaluator.implies(q, p)])\n",
    "        return ExpressionEvaluator(new_expression_tree).evaluation\n",
    "\n",
    "    @staticmethod\n",
    "    def _not(arg: LogicTerminal) -> LogicTerminal:\n",
    "        match arg:\n",
    "            case LogicTerminal.F: return LogicTerminal.T\n",
    "            case LogicTerminal.T: return LogicTerminal.F\n",
    "            case _: return LogicTerminal.U\n",
    "\n",
    "    @staticmethod\n",
    "    def _and(args: List[LogicTerminal]) -> LogicTerminal:\n",
    "        \"\"\"\n",
    "        True IFF all entries are True. If at least one is false, false, if at least one is unknown, unknown\n",
    "        \"\"\"\n",
    "        if any(arg is LogicTerminal.F for arg in args): return LogicTerminal.F\n",
    "        if any(arg is LogicTerminal.U for arg in args): return LogicTerminal.U\n",
    "        return LogicTerminal.T\n",
    "\n",
    "    @staticmethod\n",
    "    def _or(args: List[LogicTerminal]) -> LogicTerminal:\n",
    "        \"\"\"\n",
    "        True IFF any entries are True. Then Unknown if any are unknown, false otherwise\n",
    "        \"\"\"\n",
    "        if any(arg is LogicTerminal.T for arg in args): return LogicTerminal.T\n",
    "        if any(arg is LogicTerminal.U for arg in args): return LogicTerminal.U\n",
    "        return LogicTerminal.F\n",
    "\n",
    "    @staticmethod\n",
    "    def _xor(args: List[LogicTerminal]) -> LogicTerminal:\n",
    "        \"\"\"\n",
    "        True if only one entry is true, with no unknown entries.\n",
    "        If the number of true args is <=1 then the evaluation can be unknown if unknown is present.\n",
    "        Otherwise false\n",
    "        \"\"\"\n",
    "        true_count = sum(int(bool(arg)) for arg in args)\n",
    "        if true_count > 1:\n",
    "            return LogicTerminal.F\n",
    "        if LogicTerminal.U in args:\n",
    "            return LogicTerminal.U\n",
    "        return LogicTerminal.T if true_count == 1 else LogicTerminal.F\n",
    "\n",
    "\n",
    "\n",
    "    def eval(self, node: Term) -> LogicTerminal:\n",
    "        # Constants / raw terminals\n",
    "        if isinstance(node, Constant): return node.value\n",
    "        if isinstance(node, LogicTerminal): return node\n",
    "\n",
    "        # Propositional variable alone\n",
    "        if isinstance(node, Variable):\n",
    "            return self._eval_variable(node)\n",
    "\n",
    "        # Predicates (possibly with vars)\n",
    "        if isinstance(node, Predicate):\n",
    "            return self._eval_predicate(node)\n",
    "\n",
    "        # Unary NOT\n",
    "        if isinstance(node, Not):\n",
    "            return self._not(self.eval(node.child))\n",
    "\n",
    "        # Operators (AND/OR/XOR/IMPLIES/IFF)\n",
    "        if isinstance(node, Operator):\n",
    "            op = node.nodeType\n",
    "            vals = [self.eval(c) for c in node.children]\n",
    "            if op is LogicOperator.AND:     return self._and(vals)\n",
    "            if op is LogicOperator.OR:      return self._or(vals)\n",
    "            if op is LogicOperator.XOR:     return self._xor(vals)\n",
    "            if op is LogicOperator.IMPLIES: # reduce here just in case\n",
    "                if len(vals) != 2:\n",
    "                    return LogicTerminal.U\n",
    "                return self._or([self._not(vals[0]), vals[1]])\n",
    "            if op is LogicOperator.IFF:\n",
    "                if len(vals) != 2:\n",
    "                    return LogicTerminal.U\n",
    "                # (a↔b) := (a->b) ∧ (b->a)\n",
    "                ab = self._or([self._not(vals[0]), vals[1]])\n",
    "                ba = self._or([self._not(vals[1]), vals[0]])\n",
    "                return self._and([ab, ba])\n",
    "            raise ValueError(f\"Unsupported operator: {op}\")\n",
    "\n",
    "        # Quantifiers\n",
    "        if isinstance(node, QuantifierExpression):\n",
    "            return self._eval_quantifier(node)\n",
    "\n",
    "        # Fallback\n",
    "        raise ValueError(f\"Cannot evaluate node of type {type(node).__name__}\")\n",
    "\n",
    "    def _eval_variable(self, variable: Variable) -> LogicTerminal:\n",
    "        if variable.name in self.bindings:\n",
    "            bound = self.bindings[variable.name]\n",
    "            if isinstance(bound, bool):\n",
    "                return LogicTerminal.T if bound else LogicTerminal.F\n",
    "            if isinstance(bound, LogicTerminal):\n",
    "                return bound\n",
    "            return LogicTerminal.U\n",
    "        # Propositional variable lookup by name\n",
    "        return self.variable_environment.get(str(variable.name), LogicTerminal.U)\n",
    "\n",
    "    def _eval_predicate(self, predicate: Predicate) -> LogicTerminal:\n",
    "        # replace Variable by their bound values if present.\n",
    "        predicate_args: Tuple[Any, ...] = tuple(self.bindings.get(arg.name, arg) if isinstance(arg, Variable) else arg for arg in predicate.args)\n",
    "\n",
    "        # lookup in predicate table\n",
    "        key = (predicate.name, predicate_args)\n",
    "        if key in self.predicate_table:\n",
    "            return self.predicate_table[key]\n",
    "\n",
    "        # unknown if lookups do not yield truth value\n",
    "        return LogicTerminal.U\n",
    "\n",
    "    def _eval_quantifier(self, quantifier: QuantifierExpression) -> LogicTerminal:\n",
    "        # get domain\n",
    "        if isinstance(quantifier.domain, str):\n",
    "            domain_iterable = self._resolve_domain(quantifier.domain)\n",
    "        else:\n",
    "            domain_iterable = quantifier.domain\n",
    "\n",
    "        variables = list(quantifier.variables)\n",
    "        if not variables:\n",
    "            return self.eval(quantifier.expression)\n",
    "\n",
    "        # tries combinations of facts to find truth value\n",
    "        def assign_and_eval(arg_index: int) -> LogicTerminal:\n",
    "            if arg_index == len(variables):\n",
    "                return self.eval(quantifier.expression)  # all variables bound\n",
    "            variable = variables[arg_index]\n",
    "            result_accumulator: List[LogicTerminal] = []\n",
    "            for element in domain_iterable:\n",
    "                self.bindings[variable.name] = element\n",
    "                value = assign_and_eval(arg_index + 1) # Recurse and assign next value\n",
    "                result_accumulator.append(value)\n",
    "\n",
    "                if quantifier.quantifier is Quantifier.ALL and value is LogicTerminal.F:\n",
    "                    del self.bindings[variable.name]\n",
    "                    return LogicTerminal.F\n",
    "                if quantifier.quantifier is Quantifier.ANY and value is LogicTerminal.T:\n",
    "                    del self.bindings[variable.name]\n",
    "                    return LogicTerminal.T\n",
    "\n",
    "            # Clean binding for this variable\n",
    "            if variable.name in self.bindings:\n",
    "                del self.bindings[variable.name]\n",
    "\n",
    "            # Aggregate Unknowns\n",
    "            if quantifier.quantifier is Quantifier.ALL:\n",
    "                return self._and(result_accumulator)\n",
    "            else:\n",
    "                return self._or(result_accumulator)\n",
    "\n",
    "        return assign_and_eval(0)\n",
    "\n",
    "\n",
    "# </editor-fold>\n",
    "\n",
    "from typing import Set\n",
    "\n",
    "\n",
    "# <editor-fold desc=\"Wumpis World\">\n",
    "class Safety(Enum):\n",
    "    SAFE = auto()\n",
    "    RISKY = auto()\n",
    "    UNSAFE = auto()\n",
    "    UNKNOWN = auto()\n",
    "\n",
    "class PuzzleParser:\n",
    "    def __init__(self):\n",
    "        self.size: Tuple[int,int] = (-1, -1)\n",
    "        self.arrows: int = -1\n",
    "        self.path: Dict[Tuple[int, int], Dict[str, bool]] = {} # Relates Position to boolean values of Breeze and Stench\n",
    "        self.query:Tuple = (-1,-1)\n",
    "        self.resolution: Safety = Safety.UNKNOWN\n",
    "        self.file_read = False\n",
    "\n",
    "        try:\n",
    "            self.parse_puzzle()\n",
    "            self.file_read = True\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {filepath} not found\")\n",
    "            self.file_read = False\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            print(f\"Bad File: {filepath}\")\n",
    "            self.file_read = False\n",
    "\n",
    "    def __bool__(self):\n",
    "        return self.file_read\n",
    "\n",
    "    def parse_puzzle(self):\n",
    "        with open(filepath) as file:\n",
    "            path: List[str] = []\n",
    "            for raw in file.readlines():\n",
    "                line = raw.strip()\n",
    "                if line.startswith('GRID: '):\n",
    "                    grid = line.replace('GRID: ', '')\n",
    "                    self.size = tuple(map(int, grid.split('x')))\n",
    "                if line.startswith('ARROWS: '):\n",
    "                    self.arrows = int(line.replace('ARROWS: ', ''))\n",
    "                if line.startswith('QUERY: '):\n",
    "                    query = line.replace('QUERY: (', '')[:-1]\n",
    "                    self.query = tuple(map(int, query.split(',')))\n",
    "                if line.startswith('RESOLUTION: '):\n",
    "                    self.resolution = Safety[line.replace('RESOLUTION: ', '')]\n",
    "                if line.startswith('('):\n",
    "                    path.append(line)\n",
    "\n",
    "            for step in path:\n",
    "                position, breeze, stench = tuple(step.split())\n",
    "                position = position[1:-1]\n",
    "                row, col = tuple(map(int, position.split(',')))\n",
    "                breeze = breeze[-1] == 'T'\n",
    "                stench = stench[-1] == 'T'\n",
    "                self.path[(row, col)] = {\"Breeze\": breeze, \"Stench\": stench}\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.size\n",
    "\n",
    "    def get_path(self):\n",
    "        return self.path\n",
    "\n",
    "\n",
    "\n",
    "class KnowledgeBase:\n",
    "    def __init__(self):\n",
    "        self.rules: Set[FrozenSet[Term]] = set() # Each tuple is a set of disjuncts\n",
    "        self.facts: Set[FrozenSet[Term]] = set()\n",
    "\n",
    "        self.puzzle = PuzzleParser()\n",
    "        if not self.puzzle: return\n",
    "\n",
    "        self.logic_parser = Parser()\n",
    "\n",
    "        # Safe ⇒ ¬Pit and ¬Wumpus\n",
    "        self.add_rule(\"NOT Safe(x)\", \"NOT Pit(x)\")\n",
    "        self.add_rule(\"NOT Safe(x)\", \"NOT Wumpus(x)\")\n",
    "\n",
    "        # (¬Pit ∧ ¬Wumpus) ⇒ Safe  as one CNF clause:\n",
    "        self.add_rule(\"Safe(x)\", \"Pit(x)\", \"Wumpus(x)\")\n",
    "\n",
    "        self.get_puzzle_facts()\n",
    "\n",
    "    def __contains__(self, item: Tuple[Term]) -> bool:\n",
    "        clauses = self.facts | self.rules\n",
    "        if not isinstance(item, (set,tuple,list)):\n",
    "            return frozenset([item,]) in clauses\n",
    "        if isinstance(item, frozenset):\n",
    "            return item in clauses\n",
    "        return frozenset(item) in clauses\n",
    "\n",
    "    def add_rule(self,*rule):\n",
    "        \"\"\"\n",
    "        :param rule: accepts any number of arguments, these are taken as disjuncts to each other\n",
    "        \"\"\"\n",
    "        new_rule = []\n",
    "        for arg in rule:\n",
    "            if isinstance(arg, str):\n",
    "                new_rule.append(self.logic_parser(arg))\n",
    "            elif isinstance(arg, Term):\n",
    "                new_rule.append(arg)\n",
    "            elif isinstance(arg, (set,tuple,list,frozenset)):\n",
    "                new_rule.extend(list(arg))\n",
    "\n",
    "        self.rules.add(frozenset(new_rule))\n",
    "\n",
    "    def add_fact(self, *fact):\n",
    "        \"\"\"\n",
    "        :param fact: accepts any number of arguments, these are taken as disjuncts to each other\n",
    "        \"\"\"\n",
    "        new_fact = []\n",
    "        for arg in fact:\n",
    "            if isinstance(arg, str):\n",
    "                new_fact.append(self.logic_parser(arg))\n",
    "            elif isinstance(arg, Term):\n",
    "                new_fact.append(arg)\n",
    "            elif isinstance(arg, (set,tuple,list,frozenset)):\n",
    "                new_fact.extend(list(arg))\n",
    "\n",
    "        self.facts.add(frozenset(new_fact))\n",
    "\n",
    "\n",
    "    def get_neighbors(self,square: tuple)->List[Tuple[int,int]]:\n",
    "        neighbors: List[Tuple[int,int]] = []\n",
    "        xbounds,ybounds = zip((0,0),self.puzzle.get_size())\n",
    "\n",
    "        for diff in [(1,0),(-1,0),(0,1),(0,-1)]:\n",
    "            neighbor = (square[0] + diff[0], square[1] + diff[1])\n",
    "            if neighbor[0] < xbounds[0] or neighbor[0] >= xbounds[1]:\n",
    "                continue\n",
    "            if neighbor[1] < ybounds[0] or neighbor[1] >= ybounds[1]:\n",
    "                continue\n",
    "            neighbors.append(neighbor)\n",
    "\n",
    "        return neighbors\n",
    "\n",
    "    def get_puzzle_facts(self):\n",
    "        path = self.puzzle.get_path()\n",
    "        for key in path:\n",
    "            self.add_fact(Predicate(\"Safe\",(Constant(value=key),)))\n",
    "            for sense in [\"Stench\", \"Breeze\"]:\n",
    "                fact = Predicate(sense, (Constant(value=key),))\n",
    "                if path[key][sense]:\n",
    "                    self.add_fact(fact)\n",
    "                else:\n",
    "                    self.add_fact(Not(fact))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InferenceEngine:\n",
    "    # ---- fresh symbol counter for standardize-apart ----\n",
    "    _fresh_id = 0\n",
    "    @classmethod\n",
    "    def _generate_symbol(cls) -> int:\n",
    "        cls._fresh_id += 1\n",
    "        return cls._fresh_id\n",
    "\n",
    "    def __init__(self):\n",
    "        self.kb = KnowledgeBase()\n",
    "        self.add_neighbor_info()                      # Stench/Breeze → local hazard constraints\n",
    "        self.add_at_most_k_wumpus(self.kb.puzzle.arrows)  # Global cardinality: ≤ arrows\n",
    "        self.answer = self.query(self.kb.puzzle.query)\n",
    "        print(self.answer)\n",
    "\n",
    "    # ---------- neighbor propagation ----------\n",
    "    def add_neighbor_info(self):\n",
    "        hazards = {\"Stench\": \"Wumpus\", \"Breeze\": \"Pit\"}\n",
    "        new_disjuncts = []\n",
    "\n",
    "        for disjunct in self.kb.facts:\n",
    "            if len(disjunct) != 1:\n",
    "                continue\n",
    "\n",
    "            lit = next(iter(disjunct))\n",
    "            negated = isinstance(lit, Not)\n",
    "            fact = lit.child if negated else lit\n",
    "\n",
    "            if not isinstance(fact, Predicate):\n",
    "                continue\n",
    "            if fact.name not in hazards:\n",
    "                continue\n",
    "\n",
    "            cell = fact.args[0]\n",
    "            if not (isinstance(cell, Constant) and isinstance(cell.value, tuple)):\n",
    "                continue  # propagate only from ground cells\n",
    "\n",
    "            neighbors = self.kb.get_neighbors(cell.value)\n",
    "            hazard = hazards[fact.name]\n",
    "\n",
    "            if negated:\n",
    "                # ¬Sense(x) ⇒ for all neighbors y: ¬Hazard(y)\n",
    "                for n in neighbors:\n",
    "                    new_disjuncts.append(Not(Predicate(hazard, (Constant(n),))))\n",
    "            else:\n",
    "                # Sense(x) ⇒ at least one neighbor has Hazard\n",
    "                new_disjuncts.append(tuple(Predicate(hazard, (Constant(n),)) for n in neighbors))\n",
    "\n",
    "        for clause in new_disjuncts:\n",
    "            self.kb.add_fact(clause)\n",
    "\n",
    "    # ---------- candidate extraction & ≤K Wumpus encoding ----------\n",
    "    def _candidate_wumpus_cells(self) -> list[tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Cells that could still host a Wumpus given Stench/¬Stench facts:\n",
    "          - include neighbors of any Stench:T cell\n",
    "          - exclude neighbors of any Stench:F cell\n",
    "        \"\"\"\n",
    "        maybe: set[tuple[int, int]] = set()\n",
    "        ruled_out: set[tuple[int, int]] = set()\n",
    "\n",
    "        for clause in self.kb.facts:\n",
    "            if len(clause) != 1:\n",
    "                continue\n",
    "            lit = next(iter(clause))\n",
    "            is_neg = isinstance(lit, Not)\n",
    "            pred = lit.child if is_neg else lit\n",
    "            if not isinstance(pred, Predicate) or pred.name != \"Stench\":\n",
    "                continue\n",
    "\n",
    "            arg = pred.args[0]\n",
    "            if not (isinstance(arg, Constant) and isinstance(arg.value, tuple)):\n",
    "                continue\n",
    "\n",
    "            cell = arg.value\n",
    "            neigh = self.kb.get_neighbors(cell)\n",
    "            if is_neg:\n",
    "                ruled_out.update(neigh)   # ¬Stench → neighbors cannot be Wumpus\n",
    "            else:\n",
    "                maybe.update(neigh)       # Stench → some neighbor(s) must be Wumpus\n",
    "\n",
    "        maybe.difference_update(ruled_out)\n",
    "        return sorted(maybe)\n",
    "\n",
    "    def add_at_most_k_wumpus(self, k: int):\n",
    "        dom = self._candidate_wumpus_cells()\n",
    "        if len(dom) <= k:\n",
    "            return\n",
    "        for group in combinations(dom, k + 1):\n",
    "            clause = [Not(Predicate(\"Wumpus\", (Constant(c),))) for c in group]\n",
    "            self.kb.add_rule(clause)\n",
    "\n",
    "    def occurs(self, variable: Variable, term: Term, theta: Dict[Term, Term]) -> bool:\n",
    "        term = self.substitute(term, theta)\n",
    "        if variable == term:\n",
    "            return True\n",
    "        if isinstance(term, Predicate):\n",
    "            return any(self.occurs(variable, a, theta) for a in term.args)\n",
    "        return False\n",
    "\n",
    "    def substitute(self, term: Term, theta: Dict[Term, Term]) -> Term:\n",
    "        if isinstance(term, Variable) and term in theta:\n",
    "            return self.substitute(theta[term], theta)\n",
    "        if isinstance(term, Constant):\n",
    "            return term\n",
    "        if isinstance(term, Not):\n",
    "            return Not(self.substitute(term.child, theta))\n",
    "        if isinstance(term, Predicate):\n",
    "            new_args = tuple(self.substitute(a, theta) for a in term.args)\n",
    "            return Predicate(term.name, new_args)\n",
    "        return term\n",
    "\n",
    "    def unify_var(self, variable: Term, term: Term, theta: Dict[Term, Term]) -> Optional[Dict[Term, Term]]:\n",
    "        term = self.substitute(term, theta)\n",
    "        if variable == term:\n",
    "            return theta\n",
    "        if self.occurs(variable, term, theta):\n",
    "            return None\n",
    "        theta[variable] = term\n",
    "        return theta\n",
    "\n",
    "    def unify(self, a: Term, b: Term, theta: Optional[Dict[Term, Term]] = None) -> Optional[Dict[Term, Term]]:\n",
    "        if theta is None:\n",
    "            theta = {}\n",
    "        a = self.substitute(a, theta)\n",
    "        b = self.substitute(b, theta)\n",
    "        if a == b:\n",
    "            return theta\n",
    "        if isinstance(a, Variable):\n",
    "            return self.unify_var(a, b, theta)\n",
    "        if isinstance(b, Variable):\n",
    "            return self.unify_var(b, a, theta)\n",
    "        if isinstance(a, Not) and isinstance(b, Not):\n",
    "            return self.unify(a.child, b.child, theta)\n",
    "        if isinstance(a, Predicate) and isinstance(b, Predicate):\n",
    "            if a.name != b.name or len(a.args) != len(b.args):\n",
    "                return None\n",
    "            for ai, bi in zip(a.args, b.args):\n",
    "                theta = self.unify(ai, bi, theta)\n",
    "                if theta is None:\n",
    "                    return None\n",
    "            return theta\n",
    "        if isinstance(a, Constant) and isinstance(b, Constant):\n",
    "            return theta if a == b else None\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def complements(a: Term, b: Term):\n",
    "        if isinstance(a, Not) and not isinstance(b, Not):\n",
    "            return b, a.child\n",
    "        if isinstance(b, Not) and not isinstance(a, Not):\n",
    "            return a, b.child\n",
    "        return None\n",
    "\n",
    "    def _standardize_apart(self, clause: frozenset) -> frozenset:\n",
    "        \"\"\"Rename all variables in a clause uniquely\"\"\"\n",
    "        vmap: Dict[Variable, Variable] = {}\n",
    "\n",
    "        def rename(t: Term) -> Term:\n",
    "            if isinstance(t, Variable):\n",
    "                if t not in vmap:\n",
    "                    vmap[t] = Variable(name=f\"{t.name}_{self._generate_symbol()}\")\n",
    "                return vmap[t]\n",
    "            if isinstance(t, Constant):\n",
    "                return Constant(t.value)\n",
    "            if isinstance(t, Not):\n",
    "                return Not(rename(t.child))\n",
    "            if isinstance(t, Predicate):\n",
    "                return Predicate(t.name, tuple(rename(a) for a in t.args))\n",
    "            return t\n",
    "\n",
    "        return frozenset(rename(lit) for lit in clause)\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_tautology(clause: frozenset) -> bool:\n",
    "        \"\"\"Detect L ∨ ¬L inside the same clause.\"\"\"\n",
    "        seen = set()\n",
    "        for lit in clause:\n",
    "            key = (\"NOT\", lit.child) if isinstance(lit, Not) else (\"POS\", lit)\n",
    "            if key[0] == \"POS\" and (\"NOT\", key[1]) in seen:\n",
    "                return True\n",
    "            if key[0] == \"NOT\" and (\"POS\", key[1]) in seen:\n",
    "                return True\n",
    "            seen.add(key)\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def _subsumes(a: frozenset, b: frozenset) -> bool:\n",
    "        \"\"\"a ⊆ b means b is redundant given a.\"\"\"\n",
    "        return a.issubset(b)\n",
    "\n",
    "    # ------------------ resolve + main resolution ------------------\n",
    "    def resolve_clauses(self, clause1: frozenset, clause2: frozenset):\n",
    "        # Standardize apart to avoid variable collisions\n",
    "        clause1 = self._standardize_apart(clause1)\n",
    "        clause2 = self._standardize_apart(clause2)\n",
    "\n",
    "        resolvents = []\n",
    "        for L in clause1:\n",
    "            for R in clause2:\n",
    "                comps = self.complements(L, R)\n",
    "                if comps is None:\n",
    "                    continue\n",
    "                pos, neg = comps\n",
    "                theta = self.unify(pos, neg)\n",
    "                if theta is None:\n",
    "                    continue\n",
    "\n",
    "                # remove the resolved literals by identity, then substitute θ\n",
    "                merged = []\n",
    "                skip = {id(L), id(R)}\n",
    "                for t in list(clause1) + list(clause2):\n",
    "                    if id(t) in skip:\n",
    "                        continue\n",
    "                    merged.append(self.substitute(t, theta))\n",
    "\n",
    "                resolvent = frozenset(merged)\n",
    "                if not self._is_tautology(resolvent):\n",
    "                    resolvents.append(resolvent)\n",
    "        return resolvents\n",
    "\n",
    "    def resolution(self, *assumptions) -> bool:\n",
    "        \"\"\"\n",
    "        Returns True if KB ∪ assumptions ⊢ ⊥ (i.e., refutation found).\n",
    "        \"\"\"\n",
    "        clauses: set[frozenset] = set(self.kb.facts) | set(self.kb.rules)\n",
    "        for a in assumptions:\n",
    "            if isinstance(a, (list, tuple, set, frozenset)):\n",
    "                clauses.add(frozenset(a))\n",
    "            else:\n",
    "                clauses.add(frozenset((a,)))\n",
    "\n",
    "        MAX_CLAUSES = 20000  # pragmatic safety valve\n",
    "\n",
    "        while True:\n",
    "            new_resolutions: set[frozenset] = set()\n",
    "            clause_list = list(clauses)\n",
    "\n",
    "            # Subsumption pruning: keep a minimal set\n",
    "            clause_list.sort(key=len)\n",
    "            pruned: list[frozenset] = []\n",
    "            for c in clause_list:\n",
    "                if any(self._subsumes(p, c) for p in pruned):\n",
    "                    continue\n",
    "                pruned.append(c)\n",
    "            clause_list = pruned\n",
    "\n",
    "            # Pairwise resolve\n",
    "            for i in range(len(clause_list)):\n",
    "                for j in range(i + 1, len(clause_list)):\n",
    "                    for res in self.resolve_clauses(clause_list[i], clause_list[j]):\n",
    "                        if len(res) == 0:\n",
    "                            return True  # derived empty clause\n",
    "                        if res not in clauses:\n",
    "                            new_resolutions.add(res)\n",
    "                            if len(clauses) + len(new_resolutions) > MAX_CLAUSES:\n",
    "                                return False  # stop explosion → treat as no refutation\n",
    "\n",
    "            if not new_resolutions:\n",
    "                return False\n",
    "            clauses |= new_resolutions\n",
    "\n",
    "    # ------------------ pretty-print ------------------\n",
    "    def print_resolutions(self, resolutions):\n",
    "        for resolution in resolutions:\n",
    "            for clause in resolution:\n",
    "                print(clause, end=' ')\n",
    "            print()\n",
    "\n",
    "    # ------------------ query API ------------------\n",
    "    def query(self, cell: Tuple[int, int]) -> Safety:\n",
    "        safe = Predicate(\"Safe\", (Constant(cell),))\n",
    "        unsafe = Not(Predicate(\"Safe\", (Constant(cell),)))\n",
    "\n",
    "        if safe in self.kb:\n",
    "            return Safety.SAFE\n",
    "        if unsafe in self.kb:\n",
    "            return Safety.UNSAFE\n",
    "\n",
    "        if self.resolution(safe):   # KB ∧ Safe ⟹ ⊥ ⇒ KB ⊨ ¬Safe\n",
    "            return Safety.UNSAFE\n",
    "        if self.resolution(unsafe): # KB ∧ ¬Safe ⟹ ⊥ ⇒ KB ⊨ Safe\n",
    "            return Safety.SAFE\n",
    "        return Safety.RISKY\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class OutputWriter:\n",
    "\n",
    "\n",
    "    def __init__(self, filepath: str):\n",
    "        self.filepath = filepath\n",
    "\n",
    "        buffer = io.StringIO()\n",
    "        sys_stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "\n",
    "        try:\n",
    "            engine = InferenceEngine()\n",
    "            kb = engine.kb\n",
    "            try:\n",
    "                engine.run(filepath)\n",
    "            except AttributeError:\n",
    "                pass\n",
    "        finally:\n",
    "            sys.stdout = sys_stdout\n",
    "\n",
    "        self.console_output = buffer.getvalue()\n",
    "\n",
    "        # Save to file immediately\n",
    "        self.write_result(kb, self.console_output)\n",
    "\n",
    "    def _format_clause(self, clause):\n",
    "        try:\n",
    "            if isinstance(clause, (list, tuple)):\n",
    "                return \" \".join(str(c) for c in clause if c)\n",
    "            return str(clause)\n",
    "        except Exception:\n",
    "            return str(clause)\n",
    "\n",
    "    def _detect_final_resolution(self, text: str) -> str:\n",
    "        matches = re.findall(r\"\\b(SAFE|UNSAFE|RISKY)\\b\", text, re.IGNORECASE)\n",
    "        return matches[-1].upper() if matches else \"UNKNOWN\"\n",
    "\n",
    "\n",
    "    def write_result(self, kb, raw_output: str):\n",
    "        os.makedirs(\"Output\", exist_ok=True)\n",
    "\n",
    "        base_name = os.path.splitext(os.path.basename(self.filepath))[0]\n",
    "        output_file = os.path.join(\"Output\", f\"{base_name}_output.txt\")\n",
    "\n",
    "        # Metrics\n",
    "        engine = getattr(kb, \"engine\", None)\n",
    "        metrics = getattr(engine, \"metrics\", {}) if engine else {}\n",
    "        num_facts = len(getattr(kb, \"facts\", []))\n",
    "        num_rules = len(getattr(kb, \"rules\", []))\n",
    "        num_lines = len(raw_output.strip().splitlines())\n",
    "\n",
    "        result = self._detect_final_resolution(raw_output)\n",
    "\n",
    "        # Write the file\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"===== Wumpus World Logical Inference Report =====\\n\")\n",
    "            f.write(f\"Puzzle File: {self.filepath}\\n\\n\")\n",
    "\n",
    "            # METRICS\n",
    "            f.write(\"== METRICS ==\\n\")\n",
    "            f.write(f\"Facts in KB: {num_facts}\\n\")\n",
    "            f.write(f\"Rules in KB: {num_rules}\\n\")\n",
    "            f.write(f\"Output Lines: {num_lines}\\n\")\n",
    "            f.write(f\"Resolutions: {metrics.get('resolutions', 'N/A')}\\n\")\n",
    "            f.write(f\"Unifications: {metrics.get('unifications', 'N/A')}\\n\\n\")\n",
    "\n",
    "            # FACTS\n",
    "            f.write(\"== FACTS USED ==\\n\")\n",
    "            for clause in getattr(kb, \"facts\", []):\n",
    "                f.write(f\"- {self._format_clause(clause)}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            # RULES\n",
    "            f.write(\"== RULES / CLAUSES ==\\n\")\n",
    "            for clause in getattr(kb, \"rules\", []):\n",
    "                f.write(f\"- {self._format_clause(clause)}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            # RAW OUTPUT\n",
    "            f.write(\"== RAW ENGINE TRACE ==\\n\")\n",
    "            f.write(raw_output.strip() + \"\\n\\n\")\n",
    "\n",
    "            # FINAL QUERY RESULT\n",
    "            f.write(\"== FINAL QUERY RESULT ==\\n\")\n",
    "            f.write(f\"QUERY: {result}\\n\")\n",
    "\n",
    "        print(f\"[OutputWriter] Report automatically saved to {output_file}\")\n",
    "\n",
    "# </editor-fold>\n",
    "\n",
    "def run_one(p: Path, *, quiet: bool = True):\n",
    "    \"\"\"\n",
    "    Run the inference engine on a single puzzle file.\n",
    "    Returns: dict with file, expected, observed, elapsed, ok.\n",
    "    \"\"\"\n",
    "    global filepath\n",
    "    filepath = str(p)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        engine = InferenceEngine()   # builds KB, adds neighbor info, applies ≤K, answers once\n",
    "        observed = engine.answer     # Safety enum\n",
    "        expected = engine.kb.puzzle.resolution  # Safety from file\n",
    "        ok = (observed == expected)\n",
    "        elapsed = time.perf_counter() - t0\n",
    "        if not quiet:\n",
    "            print(f\"[{'PASS' if ok else 'FAIL'}] {p.name}  expected={expected.name}  got={observed.name}  ({elapsed:.3f}s)\")\n",
    "        return {\n",
    "            \"file\": str(p),\n",
    "            \"expected\": expected,\n",
    "            \"observed\": observed,\n",
    "            \"elapsed\": elapsed,\n",
    "            \"ok\": ok,\n",
    "        }\n",
    "    except FileNotFoundError:\n",
    "        elapsed = time.perf_counter() - t0\n",
    "        if not quiet:\n",
    "            print(f\"[ERROR] Not found: {p}\")\n",
    "        return {\n",
    "            \"file\": str(p),\n",
    "            \"expected\": None,\n",
    "            \"observed\": None,\n",
    "            \"elapsed\": elapsed,\n",
    "            \"ok\": False,\n",
    "            \"error\": \"FileNotFoundError\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        elapsed = time.perf_counter() - t0\n",
    "        if not quiet:\n",
    "            print(f\"[ERROR] {p.name}: {e}\")\n",
    "        return {\n",
    "            \"file\": str(p),\n",
    "            \"expected\": None,\n",
    "            \"observed\": None,\n",
    "            \"elapsed\": elapsed,\n",
    "            \"ok\": False,\n",
    "            \"error\": repr(e),\n",
    "        }\n",
    "\n",
    "def test_all(\n",
    "    root: str = \"Caves\",\n",
    "    levels: Iterable[str] = (\"easy\", \"medium\", \"hard\"),\n",
    "    pattern: str = \"*.txt\",\n",
    "    *,\n",
    "    quiet: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Discover and run all puzzles under Caves/<level> matching pattern.\n",
    "    Prints a compact summary and returns a results dict:\n",
    "      {\n",
    "        'totals': {'count': int, 'pass': int, 'fail': int, 'errors': int, 'time': float},\n",
    "        'by_level': {level: {...same keys...}},\n",
    "        'cases': [ ... per-file dicts from run_one ... ]\n",
    "      }\n",
    "    \"\"\"\n",
    "    root_path = Path(root)\n",
    "    cases = []\n",
    "    totals = {\"count\": 0, \"pass\": 0, \"fail\": 0, \"errors\": 0, \"time\": 0.0}\n",
    "    by_level: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    for level in levels:\n",
    "        level_path = root_path / level\n",
    "        level_stats = {\"count\": 0, \"pass\": 0, \"fail\": 0, \"errors\": 0, \"time\": 0.0}\n",
    "        files = sorted(level_path.glob(pattern))\n",
    "        if not quiet:\n",
    "            print(f\"\\n=== {level.upper()} ({len(files)} files) ===\")\n",
    "        for p in files:\n",
    "            r = run_one(p, quiet=quiet)\n",
    "            cases.append(r)\n",
    "\n",
    "            level_stats[\"count\"] += 1\n",
    "            level_stats[\"time\"] += r[\"elapsed\"]\n",
    "            if \"error\" in r:\n",
    "                level_stats[\"errors\"] += 1\n",
    "            elif r[\"ok\"]:\n",
    "                level_stats[\"pass\"] += 1\n",
    "            else:\n",
    "                level_stats[\"fail\"] += 1\n",
    "\n",
    "        if not quiet:\n",
    "            print(f\" -> {level}: {level_stats['pass']}/{level_stats['count']} passed, \"\n",
    "                  f\"{level_stats['fail']} failed, {level_stats['errors']} errors  \"\n",
    "                  f\"({level_stats['time']:.2f}s)\")\n",
    "        by_level[level] = level_stats\n",
    "\n",
    "        # roll into totals\n",
    "        for k in (\"count\", \"pass\", \"fail\", \"errors\", \"time\"):\n",
    "            totals[k] += level_stats[k]\n",
    "\n",
    "    if not quiet:\n",
    "        print(\"\\n=== TOTALS ===\")\n",
    "        print(f\" -> {totals['pass']}/{totals['count']} passed, \"\n",
    "              f\"{totals['fail']} failed, {totals['errors']} errors  \"\n",
    "              f\"({totals['time']:.2f}s)\")\n",
    "\n",
    "    return {\"totals\": totals, \"by_level\": by_level, \"cases\": cases}\n",
    "\n",
    "# Optional: quick CLI entrypoint\n",
    "if __name__ == \"__main__\":\n",
    "    test_all(quiet=False)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EASY (4 files) ===\n",
      "Safety.UNSAFE\n",
      "[PASS] path_e1.txt  expected=UNSAFE  got=UNSAFE  (0.032s)\n",
      "Safety.RISKY\n",
      "[PASS] path_e2.txt  expected=RISKY  got=RISKY  (0.378s)\n",
      "Safety.SAFE\n",
      "[PASS] path_e3.txt  expected=SAFE  got=SAFE  (8.875s)\n",
      "Safety.RISKY\n",
      "[FAIL] path_e4.txt  expected=UNKNOWN  got=RISKY  (0.686s)\n",
      " -> easy: 3/4 passed, 1 failed, 0 errors  (9.97s)\n",
      "\n",
      "=== MEDIUM (4 files) ===\n",
      "Safety.RISKY\n",
      "[PASS] path_m1.txt  expected=RISKY  got=RISKY  (98.250s)\n",
      "Safety.UNSAFE\n",
      "[PASS] path_m2.txt  expected=UNSAFE  got=UNSAFE  (7.036s)\n",
      "Safety.RISKY\n",
      "[PASS] path_m3.txt  expected=RISKY  got=RISKY  (21.977s)\n",
      "Safety.RISKY\n",
      "[PASS] path_m4.txt  expected=RISKY  got=RISKY  (83.236s)\n",
      " -> medium: 4/4 passed, 0 failed, 0 errors  (210.50s)\n",
      "\n",
      "=== HARD (4 files) ===\n",
      "Safety.RISKY\n",
      "[PASS] path_h1.txt  expected=RISKY  got=RISKY  (397.652s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[78]\u001B[39m\u001B[32m, line 1292\u001B[39m\n\u001B[32m   1290\u001B[39m \u001B[38;5;66;03m# Optional: quick CLI entrypoint\u001B[39;00m\n\u001B[32m   1291\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1292\u001B[39m     \u001B[43mtest_all\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquiet\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[78]\u001B[39m\u001B[32m, line 1260\u001B[39m, in \u001B[36mtest_all\u001B[39m\u001B[34m(root, levels, pattern, quiet)\u001B[39m\n\u001B[32m   1258\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m=== \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlevel.upper()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(files)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m files) ===\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1259\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m files:\n\u001B[32m-> \u001B[39m\u001B[32m1260\u001B[39m     r = \u001B[43mrun_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquiet\u001B[49m\u001B[43m=\u001B[49m\u001B[43mquiet\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1261\u001B[39m     cases.append(r)\n\u001B[32m   1263\u001B[39m     level_stats[\u001B[33m\"\u001B[39m\u001B[33mcount\u001B[39m\u001B[33m\"\u001B[39m] += \u001B[32m1\u001B[39m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[78]\u001B[39m\u001B[32m, line 1193\u001B[39m, in \u001B[36mrun_one\u001B[39m\u001B[34m(p, quiet)\u001B[39m\n\u001B[32m   1191\u001B[39m t0 = time.perf_counter()\n\u001B[32m   1192\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1193\u001B[39m     engine = \u001B[43mInferenceEngine\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m   \u001B[38;5;66;03m# builds KB, adds neighbor info, applies ≤K, answers once\u001B[39;00m\n\u001B[32m   1194\u001B[39m     observed = engine.answer     \u001B[38;5;66;03m# Safety enum\u001B[39;00m\n\u001B[32m   1195\u001B[39m     expected = engine.kb.puzzle.resolution  \u001B[38;5;66;03m# Safety from file\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[78]\u001B[39m\u001B[32m, line 809\u001B[39m, in \u001B[36mInferenceEngine.__init__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    807\u001B[39m \u001B[38;5;28mself\u001B[39m.add_neighbor_info()                      \u001B[38;5;66;03m# Stench/Breeze → local hazard constraints\u001B[39;00m\n\u001B[32m    808\u001B[39m \u001B[38;5;28mself\u001B[39m.add_at_most_k_wumpus(\u001B[38;5;28mself\u001B[39m.kb.puzzle.arrows)  \u001B[38;5;66;03m# Global cardinality: ≤ arrows\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m809\u001B[39m \u001B[38;5;28mself\u001B[39m.answer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mkb\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpuzzle\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    810\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m.answer)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[78]\u001B[39m\u001B[32m, line 1084\u001B[39m, in \u001B[36mInferenceEngine.query\u001B[39m\u001B[34m(self, cell)\u001B[39m\n\u001B[32m   1081\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m unsafe \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.kb:\n\u001B[32m   1082\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m Safety.UNSAFE\n\u001B[32m-> \u001B[39m\u001B[32m1084\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mresolution\u001B[49m\u001B[43m(\u001B[49m\u001B[43msafe\u001B[49m\u001B[43m)\u001B[49m:   \u001B[38;5;66;03m# KB ∧ Safe ⟹ ⊥ ⇒ KB ⊨ ¬Safe\u001B[39;00m\n\u001B[32m   1085\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m Safety.UNSAFE\n\u001B[32m   1086\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.resolution(unsafe): \u001B[38;5;66;03m# KB ∧ ¬Safe ⟹ ⊥ ⇒ KB ⊨ Safe\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[78]\u001B[39m\u001B[32m, line 1047\u001B[39m, in \u001B[36mInferenceEngine.resolution\u001B[39m\u001B[34m(self, *assumptions)\u001B[39m\n\u001B[32m   1045\u001B[39m pruned: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfrozenset\u001B[39m] = []\n\u001B[32m   1046\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m clause_list:\n\u001B[32m-> \u001B[39m\u001B[32m1047\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43many\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_subsumes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpruned\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m   1048\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m   1049\u001B[39m     pruned.append(c)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[78]\u001B[39m\u001B[32m, line 1047\u001B[39m, in \u001B[36m<genexpr>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m   1045\u001B[39m pruned: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfrozenset\u001B[39m] = []\n\u001B[32m   1046\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m clause_list:\n\u001B[32m-> \u001B[39m\u001B[32m1047\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_subsumes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m pruned):\n\u001B[32m   1048\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m   1049\u001B[39m     pruned.append(c)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[78]\u001B[39m\u001B[32m, line 994\u001B[39m, in \u001B[36mInferenceEngine._subsumes\u001B[39m\u001B[34m(a, b)\u001B[39m\n\u001B[32m    991\u001B[39m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[32m    992\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_subsumes\u001B[39m(a: \u001B[38;5;28mfrozenset\u001B[39m, b: \u001B[38;5;28mfrozenset\u001B[39m) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m    993\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"a ⊆ b means b is redundant given a.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m994\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43ma\u001B[49m\u001B[43m.\u001B[49m\u001B[43missubset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 78
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
